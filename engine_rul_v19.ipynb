{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed,SimpleRNN,Dropout,GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_train.xlsx\")\n",
    "df_truth = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_truth.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>more</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20426</th>\n",
       "      <td>99</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.72</td>\n",
       "      <td>1600.39</td>\n",
       "      <td>1428.03</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.29</td>\n",
       "      <td>8123.55</td>\n",
       "      <td>8.4885</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.50</td>\n",
       "      <td>23.0425</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20427</th>\n",
       "      <td>99</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.52</td>\n",
       "      <td>1605.33</td>\n",
       "      <td>1430.32</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.27</td>\n",
       "      <td>8130.99</td>\n",
       "      <td>8.5124</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.39</td>\n",
       "      <td>22.9674</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20428</th>\n",
       "      <td>99</td>\n",
       "      <td>183</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.96</td>\n",
       "      <td>1606.95</td>\n",
       "      <td>1427.90</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.31</td>\n",
       "      <td>8126.90</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.57</td>\n",
       "      <td>23.1440</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20429</th>\n",
       "      <td>99</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>644.10</td>\n",
       "      <td>1600.20</td>\n",
       "      <td>1436.54</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.33</td>\n",
       "      <td>8125.66</td>\n",
       "      <td>8.5592</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.45</td>\n",
       "      <td>23.0478</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20430</th>\n",
       "      <td>99</td>\n",
       "      <td>185</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.93</td>\n",
       "      <td>1598.42</td>\n",
       "      <td>1421.56</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8127.53</td>\n",
       "      <td>8.5425</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.1931</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20431 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0       1      1   -0.0007   -0.0004       100  518.67  641.82  1589.70   \n",
       "1       1      2    0.0019   -0.0003       100  518.67  642.15  1591.82   \n",
       "2       1      3   -0.0043    0.0003       100  518.67  642.35  1587.99   \n",
       "3       1      4    0.0007    0.0000       100  518.67  642.35  1582.79   \n",
       "4       1      5   -0.0019   -0.0002       100  518.67  642.37  1582.85   \n",
       "...    ..    ...       ...       ...       ...     ...     ...      ...   \n",
       "20426  99    181   -0.0015   -0.0001       100  518.67  643.72  1600.39   \n",
       "20427  99    182   -0.0027   -0.0003       100  518.67  643.52  1605.33   \n",
       "20428  99    183   -0.0031   -0.0003       100  518.67  643.96  1606.95   \n",
       "20429  99    184   -0.0010   -0.0001       100  518.67  644.10  1600.20   \n",
       "20430  99    185   -0.0019   -0.0004       100  518.67  643.93  1598.42   \n",
       "\n",
       "            s4     s5  ...      s13      s14     s15   s16  s17   s18  s19  \\\n",
       "0      1400.60  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100   \n",
       "1      1403.14  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100   \n",
       "2      1404.20  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100   \n",
       "3      1401.87  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100   \n",
       "4      1406.22  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100   \n",
       "...        ...    ...  ...      ...      ...     ...   ...  ...   ...  ...   \n",
       "20426  1428.03  14.62  ...  2388.29  8123.55  8.4885  0.03  396  2388  100   \n",
       "20427  1430.32  14.62  ...  2388.27  8130.99  8.5124  0.03  393  2388  100   \n",
       "20428  1427.90  14.62  ...  2388.31  8126.90  8.5374  0.03  395  2388  100   \n",
       "20429  1436.54  14.62  ...  2388.33  8125.66  8.5592  0.03  395  2388  100   \n",
       "20430  1421.56  14.62  ...  2388.24  8127.53  8.5425  0.03  397  2388  100   \n",
       "\n",
       "         s20      s21  more  \n",
       "0      39.06  23.4190    98  \n",
       "1      39.00  23.4236    98  \n",
       "2      38.95  23.3442    98  \n",
       "3      38.88  23.3739    98  \n",
       "4      38.90  23.4044    98  \n",
       "...      ...      ...   ...  \n",
       "20426  38.50  23.0425    20  \n",
       "20427  38.39  22.9674    20  \n",
       "20428  38.57  23.1440    20  \n",
       "20429  38.45  23.0478    20  \n",
       "20430  38.49  23.1931    20  \n",
       "\n",
       "[20431 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.merge(df, df_truth, on='id')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the maximum cycle for each engine\n",
    "max_cycle_per_engine = df.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_per_engine.columns = ['id', 'max_cycle']\n",
    "\n",
    "# Step 2: Merge the maximum cycle with the df_truth to get the actual failure cycle\n",
    "df_merged = pd.merge(max_cycle_per_engine, df_truth, on='id')\n",
    "\n",
    "# Step 3: Calculate the actual failure cycle (when engine will fail)\n",
    "df_merged['failure_cycle'] = df_merged['max_cycle'] + df_merged['more']\n",
    "\n",
    "# Step 4: Merge this back with the main DataFrame to compute remaining cycles\n",
    "df = pd.merge(df, df_merged[['id', 'failure_cycle']], on='id')\n",
    "\n",
    "# Step 5: Calculate remaining cycles for each row by subtracting the current cycle from the failure cycle\n",
    "df['remaining_cycles'] = df['failure_cycle'] - df['cycle']\n",
    "df = df.drop('failure_cycle',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>remaining_cycles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
       "0   1      1   -0.0007   -0.0004       100  518.67  641.82  1589.70  1400.60   \n",
       "1   1      2    0.0019   -0.0003       100  518.67  642.15  1591.82  1403.14   \n",
       "2   1      3   -0.0043    0.0003       100  518.67  642.35  1587.99  1404.20   \n",
       "3   1      4    0.0007    0.0000       100  518.67  642.35  1582.79  1401.87   \n",
       "4   1      5   -0.0019   -0.0002       100  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "      s5  ...      s13      s14     s15   s16  s17   s18  s19    s20      s21  \\\n",
       "0  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100  39.06  23.4190   \n",
       "1  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100  39.00  23.4236   \n",
       "2  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100  38.95  23.3442   \n",
       "3  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100  38.88  23.3739   \n",
       "4  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100  38.90  23.4044   \n",
       "\n",
       "   remaining_cycles  \n",
       "0               289  \n",
       "1               288  \n",
       "2               287  \n",
       "3               286  \n",
       "4               285  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20431 entries, 0 to 20430\n",
      "Data columns (total 27 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                20431 non-null  int64  \n",
      " 1   cycle             20431 non-null  int64  \n",
      " 2   setting1          20431 non-null  float64\n",
      " 3   setting2          20431 non-null  float64\n",
      " 4   setting3          20431 non-null  int64  \n",
      " 5   s1                20431 non-null  float64\n",
      " 6   s2                20431 non-null  float64\n",
      " 7   s3                20431 non-null  float64\n",
      " 8   s4                20431 non-null  float64\n",
      " 9   s5                20431 non-null  float64\n",
      " 10  s6                20431 non-null  float64\n",
      " 11  s7                20431 non-null  float64\n",
      " 12  s8                20431 non-null  float64\n",
      " 13  s9                20431 non-null  float64\n",
      " 14  s10               20431 non-null  float64\n",
      " 15  s11               20431 non-null  float64\n",
      " 16  s12               20431 non-null  float64\n",
      " 17  s13               20431 non-null  float64\n",
      " 18  s14               20431 non-null  float64\n",
      " 19  s15               20431 non-null  float64\n",
      " 20  s16               20431 non-null  float64\n",
      " 21  s17               20431 non-null  int64  \n",
      " 22  s18               20431 non-null  int64  \n",
      " 23  s19               20431 non-null  int64  \n",
      " 24  s20               20431 non-null  float64\n",
      " 25  s21               20431 non-null  float64\n",
      " 26  remaining_cycles  20431 non-null  int64  \n",
      "dtypes: float64(20), int64(7)\n",
      "memory usage: 4.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_3(df, window_size=30, test_size=0.1, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Generates scaled sequences and splits into training, validation, and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with engine data.\n",
    "    window_size (int): Number of time steps in each sequence.\n",
    "    test_size (float): Fraction of data to reserve for testing.\n",
    "    val_size (float): Fraction of data to reserve for validation.\n",
    "\n",
    "    Returns:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, feature_scaler, target_scaler (np.ndarray): \n",
    "    Arrays of train/val/test sequences and targets, feature and target scalers.\n",
    "    \"\"\"\n",
    "    features = [col for col in df.columns if col not in ['id', 'cycle', 'remaining_cycles']]\n",
    "    target_column = 'remaining_cycles'\n",
    "    \n",
    "    # Initialize scalers\n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Lists to collect sequences and targets\n",
    "    X_sequences = []\n",
    "    y_targets = []\n",
    "    \n",
    "    # Group by engine ID and create sliding windows before splitting\n",
    "    for engine_id, engine_data in df.groupby('id'):\n",
    "        engine_data = engine_data.sort_values(by='cycle')  # Sort by cycle to keep temporal order\n",
    "        \n",
    "        # Create sliding windows\n",
    "        for i in range(len(engine_data) - window_size):\n",
    "            X_sequence = engine_data[features].iloc[i:i + window_size].values\n",
    "            y_target = engine_data[target_column].iloc[i + window_size - 1]  # Target is RUL of last cycle in window\n",
    "            \n",
    "            X_sequences.append(X_sequence)\n",
    "            y_targets.append(y_target)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_targets = np.array(y_targets)\n",
    "    \n",
    "    # Split data into training+validation and testing\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_sequences, y_targets, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Further split the training+validation data into training and validation\n",
    "    val_ratio_adjusted = val_size / (1 - test_size)  # Adjust validation size proportionally\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_ratio_adjusted, random_state=42)\n",
    "    \n",
    "    # Fit the scalers on the training data\n",
    "    feature_scaler.fit(X_train.reshape(-1, X_train.shape[2]))  # Reshape for fitting scaler\n",
    "    target_scaler.fit(y_train.reshape(-1, 1))  # Reshape target for fitting scaler\n",
    "    \n",
    "    # Apply the scalers to the data\n",
    "    X_train = feature_scaler.transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)  # Reshape back after scaling\n",
    "    X_val = feature_scaler.transform(X_val.reshape(-1, X_val.shape[2])).reshape(X_val.shape)\n",
    "    X_test = feature_scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "    \n",
    "    y_train = target_scaler.transform(y_train.reshape(-1, 1))\n",
    "    y_val = target_scaler.transform(y_val.reshape(-1, 1))\n",
    "    y_test = target_scaler.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, feature_scaler, target_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, title,lim=(None,None)):\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.ylim(lim)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted evaluation function\n",
    "def evaluate_model(model, X_test, y_test,type,scaler):\n",
    "    predictions = model.predict(X_test)\n",
    "    if predictions.shape[0] != y_test.shape[0]:\n",
    "        # Trimming to the minimum size for consistency\n",
    "        min_len = min(predictions.shape[0], y_test.shape[0])\n",
    "        predictions = predictions[:min_len]\n",
    "        y_test = y_test[:min_len]\n",
    "    rmse = np.sqrt(mean_squared_error(scaler.inverse_transform(y_test.reshape(-1,1)), scaler.inverse_transform(predictions.reshape(-1,1))))\n",
    "    mae = mean_absolute_error(scaler.inverse_transform(y_test.reshape(-1,1)), scaler.inverse_transform(predictions.reshape(-1,1)))\n",
    "    print(f\"{type} Model Evaluation - RMSE: {rmse:.2f}, MAE: {mae:.2f}\")\n",
    "    return predictions, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_1=5\n",
    "window_size_2=10\n",
    "window_size_3=20\n",
    "window_size_4=30\n",
    "window_size_5=40\n",
    "window_size_6=50\n",
    "\n",
    "num_features=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "873/873 [==============================] - 35s 25ms/step - loss: 0.6105 - mse: 0.5824 - val_loss: 0.5824 - val_mse: 0.5286\n",
      "Epoch 2/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.6004 - mse: 0.5672 - val_loss: 0.5816 - val_mse: 0.5292\n",
      "Epoch 3/200\n",
      "873/873 [==============================] - 22s 25ms/step - loss: 0.5950 - mse: 0.5593 - val_loss: 0.5741 - val_mse: 0.5152\n",
      "Epoch 4/200\n",
      "873/873 [==============================] - 19s 21ms/step - loss: 0.5922 - mse: 0.5554 - val_loss: 0.5752 - val_mse: 0.5201\n",
      "Epoch 5/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.5870 - mse: 0.5489 - val_loss: 0.5761 - val_mse: 0.5214\n",
      "Epoch 6/200\n",
      "873/873 [==============================] - 20s 23ms/step - loss: 0.5856 - mse: 0.5472 - val_loss: 0.5675 - val_mse: 0.5101\n",
      "Epoch 7/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.5812 - mse: 0.5431 - val_loss: 0.5657 - val_mse: 0.5098\n",
      "Epoch 8/200\n",
      "873/873 [==============================] - 18s 21ms/step - loss: 0.5739 - mse: 0.5348 - val_loss: 0.5623 - val_mse: 0.5064\n",
      "Epoch 9/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.5679 - mse: 0.5289 - val_loss: 0.5564 - val_mse: 0.4966\n",
      "Epoch 10/200\n",
      "873/873 [==============================] - 19s 22ms/step - loss: 0.5636 - mse: 0.5222 - val_loss: 0.5631 - val_mse: 0.5093\n",
      "Epoch 11/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.5555 - mse: 0.5113 - val_loss: 0.5456 - val_mse: 0.4864\n",
      "Epoch 12/200\n",
      "873/873 [==============================] - 19s 22ms/step - loss: 0.5492 - mse: 0.5044 - val_loss: 0.5429 - val_mse: 0.4816\n",
      "Epoch 13/200\n",
      "873/873 [==============================] - 25s 29ms/step - loss: 0.5389 - mse: 0.4918 - val_loss: 0.5416 - val_mse: 0.4793\n",
      "Epoch 14/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.5285 - mse: 0.4787 - val_loss: 0.5377 - val_mse: 0.4723\n",
      "Epoch 15/200\n",
      "873/873 [==============================] - 25s 28ms/step - loss: 0.5209 - mse: 0.4676 - val_loss: 0.5305 - val_mse: 0.4665\n",
      "Epoch 16/200\n",
      "873/873 [==============================] - 23s 26ms/step - loss: 0.5124 - mse: 0.4566 - val_loss: 0.5283 - val_mse: 0.4628\n",
      "Epoch 17/200\n",
      "873/873 [==============================] - 22s 25ms/step - loss: 0.5041 - mse: 0.4438 - val_loss: 0.5242 - val_mse: 0.4587\n",
      "Epoch 18/200\n",
      "873/873 [==============================] - 20s 22ms/step - loss: 0.4968 - mse: 0.4330 - val_loss: 0.5311 - val_mse: 0.4728\n",
      "Epoch 19/200\n",
      "873/873 [==============================] - 20s 23ms/step - loss: 0.4901 - mse: 0.4258 - val_loss: 0.5301 - val_mse: 0.4665\n",
      "Epoch 20/200\n",
      "873/873 [==============================] - 23s 27ms/step - loss: 0.4820 - mse: 0.4133 - val_loss: 0.5144 - val_mse: 0.4473\n",
      "Epoch 21/200\n",
      "873/873 [==============================] - 20s 23ms/step - loss: 0.4722 - mse: 0.3996 - val_loss: 0.5121 - val_mse: 0.4392\n",
      "Epoch 22/200\n",
      "873/873 [==============================] - 24s 27ms/step - loss: 0.4649 - mse: 0.3898 - val_loss: 0.5119 - val_mse: 0.4392\n",
      "Epoch 23/200\n",
      "873/873 [==============================] - 20s 23ms/step - loss: 0.4601 - mse: 0.3806 - val_loss: 0.5036 - val_mse: 0.4239\n",
      "Epoch 24/200\n",
      "873/873 [==============================] - 22s 25ms/step - loss: 0.4524 - mse: 0.3706 - val_loss: 0.5046 - val_mse: 0.4290\n",
      "Epoch 25/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.4460 - mse: 0.3605 - val_loss: 0.5006 - val_mse: 0.4268\n",
      "Epoch 26/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.4390 - mse: 0.3513 - val_loss: 0.4963 - val_mse: 0.4147\n",
      "Epoch 27/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.4331 - mse: 0.3453 - val_loss: 0.4910 - val_mse: 0.4080\n",
      "Epoch 28/200\n",
      "873/873 [==============================] - 25s 29ms/step - loss: 0.4272 - mse: 0.3346 - val_loss: 0.4968 - val_mse: 0.4150\n",
      "Epoch 29/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.4224 - mse: 0.3274 - val_loss: 0.4859 - val_mse: 0.4013\n",
      "Epoch 30/200\n",
      "873/873 [==============================] - 20s 23ms/step - loss: 0.4164 - mse: 0.3206 - val_loss: 0.4995 - val_mse: 0.4239\n",
      "Epoch 31/200\n",
      "873/873 [==============================] - 26s 30ms/step - loss: 0.4091 - mse: 0.3079 - val_loss: 0.4948 - val_mse: 0.4127\n",
      "Epoch 32/200\n",
      "873/873 [==============================] - 25s 28ms/step - loss: 0.4075 - mse: 0.3075 - val_loss: 0.4819 - val_mse: 0.3969\n",
      "Epoch 33/200\n",
      "873/873 [==============================] - 25s 28ms/step - loss: 0.4050 - mse: 0.3040 - val_loss: 0.4863 - val_mse: 0.4084\n",
      "Epoch 34/200\n",
      "873/873 [==============================] - 29s 33ms/step - loss: 0.3965 - mse: 0.2907 - val_loss: 0.4805 - val_mse: 0.3946\n",
      "Epoch 35/200\n",
      "873/873 [==============================] - 31s 36ms/step - loss: 0.3948 - mse: 0.2885 - val_loss: 0.4812 - val_mse: 0.3980\n",
      "Epoch 36/200\n",
      "873/873 [==============================] - 27s 31ms/step - loss: 0.3909 - mse: 0.2840 - val_loss: 0.4976 - val_mse: 0.4196\n",
      "Epoch 37/200\n",
      "873/873 [==============================] - 26s 30ms/step - loss: 0.3864 - mse: 0.2751 - val_loss: 0.4776 - val_mse: 0.3927\n",
      "Epoch 38/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.3815 - mse: 0.2714 - val_loss: 0.4784 - val_mse: 0.3994\n",
      "Epoch 39/200\n",
      "873/873 [==============================] - 22s 25ms/step - loss: 0.3799 - mse: 0.2701 - val_loss: 0.4761 - val_mse: 0.3907\n",
      "Epoch 40/200\n",
      "873/873 [==============================] - 22s 25ms/step - loss: 0.3734 - mse: 0.2594 - val_loss: 0.4756 - val_mse: 0.3973\n",
      "Epoch 41/200\n",
      "873/873 [==============================] - 22s 25ms/step - loss: 0.3705 - mse: 0.2555 - val_loss: 0.4713 - val_mse: 0.3864\n",
      "Epoch 42/200\n",
      "873/873 [==============================] - 21s 24ms/step - loss: 0.3681 - mse: 0.2522 - val_loss: 0.4884 - val_mse: 0.4068\n",
      "Epoch 43/200\n",
      "873/873 [==============================] - 23s 26ms/step - loss: 0.3625 - mse: 0.2455 - val_loss: 0.4756 - val_mse: 0.3965\n",
      "Epoch 44/200\n",
      "873/873 [==============================] - 23s 26ms/step - loss: 0.3639 - mse: 0.2468 - val_loss: 0.4666 - val_mse: 0.3776\n",
      "Epoch 45/200\n",
      "873/873 [==============================] - 25s 29ms/step - loss: 0.3564 - mse: 0.2365 - val_loss: 0.4658 - val_mse: 0.3785\n",
      "Epoch 46/200\n",
      "873/873 [==============================] - 29s 33ms/step - loss: 0.3554 - mse: 0.2333 - val_loss: 0.4797 - val_mse: 0.3987\n",
      "Epoch 47/200\n",
      "873/873 [==============================] - 24s 27ms/step - loss: 0.3505 - mse: 0.2298 - val_loss: 0.4675 - val_mse: 0.3934\n",
      "Epoch 48/200\n",
      "873/873 [==============================] - 24s 27ms/step - loss: 0.3496 - mse: 0.2291 - val_loss: 0.4737 - val_mse: 0.3947\n",
      "Epoch 49/200\n",
      "873/873 [==============================] - 23s 26ms/step - loss: 0.3487 - mse: 0.2274 - val_loss: 0.4732 - val_mse: 0.3882\n",
      "Epoch 50/200\n",
      "873/873 [==============================] - 25s 28ms/step - loss: 0.3469 - mse: 0.2241 - val_loss: 0.4731 - val_mse: 0.3889\n",
      "Epoch 51/200\n",
      "873/873 [==============================] - 26s 30ms/step - loss: 0.3390 - mse: 0.2155 - val_loss: 0.4746 - val_mse: 0.3921\n",
      "Epoch 52/200\n",
      "873/873 [==============================] - 23s 26ms/step - loss: 0.3406 - mse: 0.2142 - val_loss: 0.4738 - val_mse: 0.3916\n",
      "Epoch 53/200\n",
      "873/873 [==============================] - 26s 30ms/step - loss: 0.3353 - mse: 0.2088 - val_loss: 0.4698 - val_mse: 0.3880\n",
      "Epoch 54/200\n",
      "873/873 [==============================] - 24s 27ms/step - loss: 0.3337 - mse: 0.2080 - val_loss: 0.4877 - val_mse: 0.4118\n",
      "Epoch 55/200\n",
      "873/873 [==============================] - 27s 31ms/step - loss: 0.3311 - mse: 0.2021 - val_loss: 0.4684 - val_mse: 0.3853\n",
      "Epoch 56/200\n",
      "873/873 [==============================] - 26s 29ms/step - loss: 0.3278 - mse: 0.2000 - val_loss: 0.4688 - val_mse: 0.3798\n",
      "Epoch 57/200\n",
      "873/873 [==============================] - 23s 27ms/step - loss: 0.3280 - mse: 0.1998 - val_loss: 0.4737 - val_mse: 0.3907\n",
      "Epoch 58/200\n",
      "873/873 [==============================] - 23s 27ms/step - loss: 0.3250 - mse: 0.1969 - val_loss: 0.4681 - val_mse: 0.3799\n",
      "Epoch 59/200\n",
      "873/873 [==============================] - 15s 18ms/step - loss: 0.3189 - mse: 0.1903 - val_loss: 0.4680 - val_mse: 0.3812\n",
      "Epoch 60/200\n",
      "873/873 [==============================] - 16s 18ms/step - loss: 0.3193 - mse: 0.1910 - val_loss: 0.4842 - val_mse: 0.4020\n",
      "Epoch 61/200\n",
      "873/873 [==============================] - 16s 18ms/step - loss: 0.3188 - mse: 0.1897 - val_loss: 0.4662 - val_mse: 0.3763\n",
      "Epoch 62/200\n",
      "873/873 [==============================] - 15s 18ms/step - loss: 0.3173 - mse: 0.1877 - val_loss: 0.4688 - val_mse: 0.3802\n",
      "Epoch 63/200\n",
      "873/873 [==============================] - 15s 17ms/step - loss: 0.3138 - mse: 0.1838 - val_loss: 0.4778 - val_mse: 0.3958\n",
      "Epoch 64/200\n",
      "873/873 [==============================] - 17s 19ms/step - loss: 0.3141 - mse: 0.1839 - val_loss: 0.4770 - val_mse: 0.4036\n",
      "Epoch 65/200\n",
      "873/873 [==============================] - 16s 19ms/step - loss: 0.3137 - mse: 0.1829 - val_loss: 0.4722 - val_mse: 0.3934\n",
      "Epoch 66/200\n",
      "873/873 [==============================] - 17s 19ms/step - loss: 0.3091 - mse: 0.1778 - val_loss: 0.4685 - val_mse: 0.3857\n",
      "Epoch 67/200\n",
      "873/873 [==============================] - 18s 21ms/step - loss: 0.3095 - mse: 0.1779 - val_loss: 0.4747 - val_mse: 0.4009\n",
      "Epoch 68/200\n",
      "873/873 [==============================] - 15s 17ms/step - loss: 0.3090 - mse: 0.1784 - val_loss: 0.4736 - val_mse: 0.3929\n",
      "Epoch 69/200\n",
      "873/873 [==============================] - 16s 18ms/step - loss: 0.3061 - mse: 0.1744 - val_loss: 0.4712 - val_mse: 0.3945\n",
      "Epoch 70/200\n",
      "873/873 [==============================] - 18s 20ms/step - loss: 0.3035 - mse: 0.1702 - val_loss: 0.4794 - val_mse: 0.3973\n",
      "Epoch 71/200\n",
      "873/873 [==============================] - 17s 20ms/step - loss: 0.3010 - mse: 0.1694 - val_loss: 0.4700 - val_mse: 0.3873\n",
      "Epoch 72/200\n",
      "873/873 [==============================] - 17s 19ms/step - loss: 0.2992 - mse: 0.1663 - val_loss: 0.4765 - val_mse: 0.3939\n",
      "Epoch 73/200\n",
      "873/873 [==============================] - 15s 17ms/step - loss: 0.2977 - mse: 0.1660 - val_loss: 0.4762 - val_mse: 0.3976\n",
      "Epoch 74/200\n",
      "873/873 [==============================] - 16s 19ms/step - loss: 0.2986 - mse: 0.1634 - val_loss: 0.4750 - val_mse: 0.3962\n",
      "Epoch 75/200\n",
      "873/873 [==============================] - 16s 18ms/step - loss: 0.2926 - mse: 0.1621 - val_loss: 0.4758 - val_mse: 0.3916\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_1, test_size=0.2)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(window_size_1, num_features)),\n",
    "    Dropout(0.3),\n",
    "    GRU(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "model_1.compile(optimizer=Adam(learning_rate=0.001),loss='mae',metrics=['mse'])\n",
    "# Train the model\n",
    "history_1 = model_1.fit(X_train,y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    epochs=200, batch_size=16,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(patience=30,monitor='val_loss',restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "851/851 [==============================] - 38s 31ms/step - loss: 0.6153 - mse: 0.5915 - val_loss: 0.5893 - val_mse: 0.5579\n",
      "Epoch 2/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.5988 - mse: 0.5675 - val_loss: 0.5900 - val_mse: 0.5634\n",
      "Epoch 3/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.5872 - mse: 0.5536 - val_loss: 0.5784 - val_mse: 0.5448\n",
      "Epoch 4/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.5729 - mse: 0.5340 - val_loss: 0.5609 - val_mse: 0.5207\n",
      "Epoch 5/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.5593 - mse: 0.5201 - val_loss: 0.5700 - val_mse: 0.5361\n",
      "Epoch 6/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.5376 - mse: 0.4891 - val_loss: 0.5528 - val_mse: 0.5355\n",
      "Epoch 7/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.5108 - mse: 0.4515 - val_loss: 0.5157 - val_mse: 0.4594\n",
      "Epoch 8/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.4846 - mse: 0.4131 - val_loss: 0.5262 - val_mse: 0.4582\n",
      "Epoch 9/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.4562 - mse: 0.3732 - val_loss: 0.4727 - val_mse: 0.3909\n",
      "Epoch 10/200\n",
      "851/851 [==============================] - 22s 25ms/step - loss: 0.4267 - mse: 0.3330 - val_loss: 0.4390 - val_mse: 0.3548\n",
      "Epoch 11/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.4038 - mse: 0.2994 - val_loss: 0.4171 - val_mse: 0.3275\n",
      "Epoch 12/200\n",
      "851/851 [==============================] - 22s 25ms/step - loss: 0.3776 - mse: 0.2643 - val_loss: 0.4335 - val_mse: 0.3313\n",
      "Epoch 13/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.3582 - mse: 0.2396 - val_loss: 0.3678 - val_mse: 0.2638\n",
      "Epoch 14/200\n",
      "851/851 [==============================] - 24s 28ms/step - loss: 0.3380 - mse: 0.2157 - val_loss: 0.3576 - val_mse: 0.2481\n",
      "Epoch 15/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.3270 - mse: 0.2017 - val_loss: 0.3475 - val_mse: 0.2386\n",
      "Epoch 16/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.3164 - mse: 0.1874 - val_loss: 0.3288 - val_mse: 0.2154\n",
      "Epoch 17/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.2986 - mse: 0.1682 - val_loss: 0.3306 - val_mse: 0.2129\n",
      "Epoch 18/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.2904 - mse: 0.1572 - val_loss: 0.3258 - val_mse: 0.1971\n",
      "Epoch 19/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.2808 - mse: 0.1467 - val_loss: 0.3177 - val_mse: 0.2004\n",
      "Epoch 20/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.2687 - mse: 0.1347 - val_loss: 0.2980 - val_mse: 0.1725\n",
      "Epoch 21/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.2621 - mse: 0.1289 - val_loss: 0.3003 - val_mse: 0.1785\n",
      "Epoch 22/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.2544 - mse: 0.1212 - val_loss: 0.2924 - val_mse: 0.1623\n",
      "Epoch 23/200\n",
      "851/851 [==============================] - 25s 30ms/step - loss: 0.2477 - mse: 0.1130 - val_loss: 0.2792 - val_mse: 0.1514\n",
      "Epoch 24/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.2406 - mse: 0.1072 - val_loss: 0.2755 - val_mse: 0.1507\n",
      "Epoch 25/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.2380 - mse: 0.1039 - val_loss: 0.2690 - val_mse: 0.1413\n",
      "Epoch 26/200\n",
      "851/851 [==============================] - 22s 25ms/step - loss: 0.2321 - mse: 0.0990 - val_loss: 0.2727 - val_mse: 0.1453\n",
      "Epoch 27/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.2245 - mse: 0.0921 - val_loss: 0.2650 - val_mse: 0.1362\n",
      "Epoch 28/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.2214 - mse: 0.0898 - val_loss: 0.2757 - val_mse: 0.1400\n",
      "Epoch 29/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.2180 - mse: 0.0863 - val_loss: 0.2492 - val_mse: 0.1224\n",
      "Epoch 30/200\n",
      "851/851 [==============================] - 22s 25ms/step - loss: 0.2119 - mse: 0.0821 - val_loss: 0.2540 - val_mse: 0.1266\n",
      "Epoch 31/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.2105 - mse: 0.0795 - val_loss: 0.2452 - val_mse: 0.1173\n",
      "Epoch 32/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.2079 - mse: 0.0773 - val_loss: 0.2723 - val_mse: 0.1369\n",
      "Epoch 33/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.2064 - mse: 0.0765 - val_loss: 0.2509 - val_mse: 0.1180\n",
      "Epoch 34/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.2002 - mse: 0.0719 - val_loss: 0.2478 - val_mse: 0.1216\n",
      "Epoch 35/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1977 - mse: 0.0707 - val_loss: 0.2412 - val_mse: 0.1130\n",
      "Epoch 36/200\n",
      "851/851 [==============================] - 24s 29ms/step - loss: 0.1939 - mse: 0.0684 - val_loss: 0.2335 - val_mse: 0.1052\n",
      "Epoch 37/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.1921 - mse: 0.0665 - val_loss: 0.2326 - val_mse: 0.1053\n",
      "Epoch 38/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1910 - mse: 0.0650 - val_loss: 0.2425 - val_mse: 0.1167\n",
      "Epoch 39/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1881 - mse: 0.0629 - val_loss: 0.2427 - val_mse: 0.1128\n",
      "Epoch 40/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1846 - mse: 0.0612 - val_loss: 0.2248 - val_mse: 0.0979\n",
      "Epoch 41/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.1837 - mse: 0.0601 - val_loss: 0.2348 - val_mse: 0.1033\n",
      "Epoch 42/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1821 - mse: 0.0591 - val_loss: 0.2311 - val_mse: 0.1078\n",
      "Epoch 43/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1783 - mse: 0.0565 - val_loss: 0.2442 - val_mse: 0.1128\n",
      "Epoch 44/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.1767 - mse: 0.0557 - val_loss: 0.2271 - val_mse: 0.1013\n",
      "Epoch 45/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.1768 - mse: 0.0554 - val_loss: 0.2339 - val_mse: 0.1070\n",
      "Epoch 46/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1739 - mse: 0.0538 - val_loss: 0.2299 - val_mse: 0.1051\n",
      "Epoch 47/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1738 - mse: 0.0537 - val_loss: 0.2200 - val_mse: 0.0971\n",
      "Epoch 48/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1732 - mse: 0.0530 - val_loss: 0.2404 - val_mse: 0.1117\n",
      "Epoch 49/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1709 - mse: 0.0520 - val_loss: 0.2305 - val_mse: 0.1035\n",
      "Epoch 50/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1695 - mse: 0.0514 - val_loss: 0.2221 - val_mse: 0.0984\n",
      "Epoch 51/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1680 - mse: 0.0497 - val_loss: 0.2244 - val_mse: 0.0966\n",
      "Epoch 52/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1648 - mse: 0.0483 - val_loss: 0.2150 - val_mse: 0.0924\n",
      "Epoch 53/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1651 - mse: 0.0479 - val_loss: 0.2172 - val_mse: 0.0908\n",
      "Epoch 54/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1643 - mse: 0.0474 - val_loss: 0.2180 - val_mse: 0.0947\n",
      "Epoch 55/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1616 - mse: 0.0456 - val_loss: 0.2121 - val_mse: 0.0880\n",
      "Epoch 56/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1601 - mse: 0.0456 - val_loss: 0.2201 - val_mse: 0.0963\n",
      "Epoch 57/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1612 - mse: 0.0456 - val_loss: 0.2129 - val_mse: 0.0892\n",
      "Epoch 58/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1614 - mse: 0.0460 - val_loss: 0.2143 - val_mse: 0.0900\n",
      "Epoch 59/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1580 - mse: 0.0438 - val_loss: 0.2256 - val_mse: 0.0999\n",
      "Epoch 60/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1565 - mse: 0.0430 - val_loss: 0.2114 - val_mse: 0.0910\n",
      "Epoch 61/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1565 - mse: 0.0428 - val_loss: 0.2188 - val_mse: 0.0932\n",
      "Epoch 62/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1545 - mse: 0.0421 - val_loss: 0.2193 - val_mse: 0.0980\n",
      "Epoch 63/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1555 - mse: 0.0423 - val_loss: 0.2227 - val_mse: 0.0963\n",
      "Epoch 64/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1554 - mse: 0.0421 - val_loss: 0.2148 - val_mse: 0.0939\n",
      "Epoch 65/200\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.1529 - mse: 0.0410 - val_loss: 0.2221 - val_mse: 0.1007\n",
      "Epoch 66/200\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.1515 - mse: 0.0400 - val_loss: 0.2109 - val_mse: 0.0877\n",
      "Epoch 67/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1527 - mse: 0.0410 - val_loss: 0.2134 - val_mse: 0.0911\n",
      "Epoch 68/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1528 - mse: 0.0412 - val_loss: 0.2141 - val_mse: 0.0905\n",
      "Epoch 69/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1526 - mse: 0.0407 - val_loss: 0.2194 - val_mse: 0.0919\n",
      "Epoch 70/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1486 - mse: 0.0386 - val_loss: 0.2134 - val_mse: 0.0876\n",
      "Epoch 71/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1485 - mse: 0.0389 - val_loss: 0.2088 - val_mse: 0.0871\n",
      "Epoch 72/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1477 - mse: 0.0383 - val_loss: 0.2195 - val_mse: 0.0929\n",
      "Epoch 73/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1454 - mse: 0.0366 - val_loss: 0.2086 - val_mse: 0.0860\n",
      "Epoch 74/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1463 - mse: 0.0374 - val_loss: 0.2167 - val_mse: 0.0905\n",
      "Epoch 75/200\n",
      "851/851 [==============================] - 19s 22ms/step - loss: 0.1479 - mse: 0.0386 - val_loss: 0.2139 - val_mse: 0.0903\n",
      "Epoch 76/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1457 - mse: 0.0367 - val_loss: 0.2180 - val_mse: 0.0934\n",
      "Epoch 77/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1430 - mse: 0.0357 - val_loss: 0.2113 - val_mse: 0.0884\n",
      "Epoch 78/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1433 - mse: 0.0362 - val_loss: 0.2111 - val_mse: 0.0885\n",
      "Epoch 79/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1428 - mse: 0.0357 - val_loss: 0.2103 - val_mse: 0.0869\n",
      "Epoch 80/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1434 - mse: 0.0357 - val_loss: 0.2203 - val_mse: 0.0975\n",
      "Epoch 81/200\n",
      "851/851 [==============================] - 19s 22ms/step - loss: 0.1423 - mse: 0.0353 - val_loss: 0.2134 - val_mse: 0.0895\n",
      "Epoch 82/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1407 - mse: 0.0343 - val_loss: 0.2196 - val_mse: 0.0947\n",
      "Epoch 83/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1391 - mse: 0.0338 - val_loss: 0.2113 - val_mse: 0.0879\n",
      "Epoch 84/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1409 - mse: 0.0351 - val_loss: 0.2109 - val_mse: 0.0918\n",
      "Epoch 85/200\n",
      "851/851 [==============================] - 20s 23ms/step - loss: 0.1402 - mse: 0.0344 - val_loss: 0.2188 - val_mse: 0.0957\n",
      "Epoch 86/200\n",
      "851/851 [==============================] - 18s 22ms/step - loss: 0.1394 - mse: 0.0340 - val_loss: 0.2088 - val_mse: 0.0866\n",
      "Epoch 87/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1384 - mse: 0.0332 - val_loss: 0.2111 - val_mse: 0.0899\n",
      "Epoch 88/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1384 - mse: 0.0333 - val_loss: 0.2065 - val_mse: 0.0836\n",
      "Epoch 89/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1375 - mse: 0.0324 - val_loss: 0.2165 - val_mse: 0.0911\n",
      "Epoch 90/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1355 - mse: 0.0319 - val_loss: 0.2032 - val_mse: 0.0843\n",
      "Epoch 91/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1386 - mse: 0.0336 - val_loss: 0.2113 - val_mse: 0.0869\n",
      "Epoch 92/200\n",
      "851/851 [==============================] - 18s 22ms/step - loss: 0.1368 - mse: 0.0328 - val_loss: 0.2106 - val_mse: 0.0891\n",
      "Epoch 93/200\n",
      "851/851 [==============================] - 19s 22ms/step - loss: 0.1368 - mse: 0.0323 - val_loss: 0.2169 - val_mse: 0.0902\n",
      "Epoch 94/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1348 - mse: 0.0317 - val_loss: 0.2083 - val_mse: 0.0855\n",
      "Epoch 95/200\n",
      "851/851 [==============================] - 22s 25ms/step - loss: 0.1347 - mse: 0.0320 - val_loss: 0.2170 - val_mse: 0.0930\n",
      "Epoch 96/200\n",
      "851/851 [==============================] - 20s 24ms/step - loss: 0.1368 - mse: 0.0327 - val_loss: 0.2158 - val_mse: 0.0926\n",
      "Epoch 97/200\n",
      "851/851 [==============================] - 21s 24ms/step - loss: 0.1345 - mse: 0.0318 - val_loss: 0.2163 - val_mse: 0.0917\n",
      "Epoch 98/200\n",
      "851/851 [==============================] - 21s 25ms/step - loss: 0.1341 - mse: 0.0316 - val_loss: 0.2056 - val_mse: 0.0871\n",
      "Epoch 99/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1330 - mse: 0.0310 - val_loss: 0.2109 - val_mse: 0.0916\n",
      "Epoch 100/200\n",
      "851/851 [==============================] - 18s 21ms/step - loss: 0.1338 - mse: 0.0308 - val_loss: 0.2129 - val_mse: 0.0911\n",
      "Epoch 101/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1330 - mse: 0.0309 - val_loss: 0.2120 - val_mse: 0.0887\n",
      "Epoch 102/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1332 - mse: 0.0309 - val_loss: 0.2219 - val_mse: 0.0948\n",
      "Epoch 103/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1320 - mse: 0.0304 - val_loss: 0.2123 - val_mse: 0.0889\n",
      "Epoch 104/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1344 - mse: 0.0313 - val_loss: 0.2165 - val_mse: 0.0939\n",
      "Epoch 105/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1327 - mse: 0.0309 - val_loss: 0.2128 - val_mse: 0.0888\n",
      "Epoch 106/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1332 - mse: 0.0309 - val_loss: 0.2122 - val_mse: 0.0890\n",
      "Epoch 107/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1313 - mse: 0.0301 - val_loss: 0.2124 - val_mse: 0.0910\n",
      "Epoch 108/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1311 - mse: 0.0301 - val_loss: 0.2056 - val_mse: 0.0878\n",
      "Epoch 109/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1295 - mse: 0.0289 - val_loss: 0.2122 - val_mse: 0.0895\n",
      "Epoch 110/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1315 - mse: 0.0303 - val_loss: 0.2076 - val_mse: 0.0883\n",
      "Epoch 111/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1300 - mse: 0.0297 - val_loss: 0.2053 - val_mse: 0.0861\n",
      "Epoch 112/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1293 - mse: 0.0292 - val_loss: 0.2110 - val_mse: 0.0900\n",
      "Epoch 113/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1291 - mse: 0.0290 - val_loss: 0.2125 - val_mse: 0.0940\n",
      "Epoch 114/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1293 - mse: 0.0290 - val_loss: 0.2061 - val_mse: 0.0888\n",
      "Epoch 115/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1302 - mse: 0.0294 - val_loss: 0.2081 - val_mse: 0.0862\n",
      "Epoch 116/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1294 - mse: 0.0288 - val_loss: 0.2113 - val_mse: 0.0886\n",
      "Epoch 117/200\n",
      "851/851 [==============================] - 17s 19ms/step - loss: 0.1295 - mse: 0.0295 - val_loss: 0.2126 - val_mse: 0.0920\n",
      "Epoch 118/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1296 - mse: 0.0291 - val_loss: 0.2129 - val_mse: 0.0906\n",
      "Epoch 119/200\n",
      "851/851 [==============================] - 17s 20ms/step - loss: 0.1284 - mse: 0.0293 - val_loss: 0.2198 - val_mse: 0.0941\n",
      "Epoch 120/200\n",
      "851/851 [==============================] - 17s 19ms/step - loss: 0.1283 - mse: 0.0286 - val_loss: 0.2143 - val_mse: 0.0919\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_2, test_size=0.2)\n",
    "\n",
    "model_2 = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(window_size_2, num_features)),\n",
    "    Dropout(0.3),\n",
    "    GRU(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "model_2.compile(optimizer=Adam(learning_rate=0.001),loss='mae',metrics=['mse'])\n",
    "# Train the model\n",
    "history_2 = model_2.fit(X_train,y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    epochs=200, batch_size=16,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(patience=30,monitor='val_loss',restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "808/808 [==============================] - 35s 33ms/step - loss: 0.6176 - mse: 0.5948 - val_loss: 0.6119 - val_mse: 0.5641\n",
      "Epoch 2/200\n",
      "808/808 [==============================] - 25s 30ms/step - loss: 0.5953 - mse: 0.5603 - val_loss: 0.5704 - val_mse: 0.5290\n",
      "Epoch 3/200\n",
      "808/808 [==============================] - 24s 30ms/step - loss: 0.5667 - mse: 0.5205 - val_loss: 0.5713 - val_mse: 0.5561\n",
      "Epoch 4/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.5283 - mse: 0.4697 - val_loss: 0.5033 - val_mse: 0.4366\n",
      "Epoch 5/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.4785 - mse: 0.4010 - val_loss: 0.4594 - val_mse: 0.3667\n",
      "Epoch 6/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.4286 - mse: 0.3340 - val_loss: 0.4351 - val_mse: 0.3240\n",
      "Epoch 7/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.3810 - mse: 0.2727 - val_loss: 0.3633 - val_mse: 0.2673\n",
      "Epoch 8/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.3408 - mse: 0.2204 - val_loss: 0.3146 - val_mse: 0.1940\n",
      "Epoch 9/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.3065 - mse: 0.1785 - val_loss: 0.3010 - val_mse: 0.1730\n",
      "Epoch 10/200\n",
      "808/808 [==============================] - 25s 30ms/step - loss: 0.2761 - mse: 0.1461 - val_loss: 0.2663 - val_mse: 0.1441\n",
      "Epoch 11/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.2516 - mse: 0.1225 - val_loss: 0.2350 - val_mse: 0.1154\n",
      "Epoch 12/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.2316 - mse: 0.1031 - val_loss: 0.2248 - val_mse: 0.1028\n",
      "Epoch 13/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.2141 - mse: 0.0876 - val_loss: 0.2153 - val_mse: 0.0925\n",
      "Epoch 14/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.2001 - mse: 0.0753 - val_loss: 0.1960 - val_mse: 0.0764\n",
      "Epoch 15/200\n",
      "808/808 [==============================] - 25s 30ms/step - loss: 0.1877 - mse: 0.0653 - val_loss: 0.1899 - val_mse: 0.0687\n",
      "Epoch 16/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1810 - mse: 0.0608 - val_loss: 0.1606 - val_mse: 0.0507\n",
      "Epoch 17/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1710 - mse: 0.0535 - val_loss: 0.1620 - val_mse: 0.0528\n",
      "Epoch 18/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1656 - mse: 0.0492 - val_loss: 0.1817 - val_mse: 0.0594\n",
      "Epoch 19/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1591 - mse: 0.0455 - val_loss: 0.1416 - val_mse: 0.0387\n",
      "Epoch 20/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1509 - mse: 0.0410 - val_loss: 0.1556 - val_mse: 0.0449\n",
      "Epoch 21/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1469 - mse: 0.0384 - val_loss: 0.1391 - val_mse: 0.0366\n",
      "Epoch 22/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1452 - mse: 0.0376 - val_loss: 0.1358 - val_mse: 0.0350\n",
      "Epoch 23/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1390 - mse: 0.0342 - val_loss: 0.1291 - val_mse: 0.0311\n",
      "Epoch 24/200\n",
      "808/808 [==============================] - 25s 30ms/step - loss: 0.1354 - mse: 0.0326 - val_loss: 0.1449 - val_mse: 0.0394\n",
      "Epoch 25/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1360 - mse: 0.0324 - val_loss: 0.1371 - val_mse: 0.0349\n",
      "Epoch 26/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1325 - mse: 0.0308 - val_loss: 0.1448 - val_mse: 0.0383\n",
      "Epoch 27/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1263 - mse: 0.0283 - val_loss: 0.1150 - val_mse: 0.0253\n",
      "Epoch 28/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1268 - mse: 0.0284 - val_loss: 0.1307 - val_mse: 0.0318\n",
      "Epoch 29/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1241 - mse: 0.0272 - val_loss: 0.1085 - val_mse: 0.0216\n",
      "Epoch 30/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1209 - mse: 0.0255 - val_loss: 0.1255 - val_mse: 0.0296\n",
      "Epoch 31/200\n",
      "808/808 [==============================] - 25s 32ms/step - loss: 0.1184 - mse: 0.0242 - val_loss: 0.1280 - val_mse: 0.0294\n",
      "Epoch 32/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.1166 - mse: 0.0239 - val_loss: 0.1100 - val_mse: 0.0224\n",
      "Epoch 33/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.1151 - mse: 0.0230 - val_loss: 0.1328 - val_mse: 0.0332\n",
      "Epoch 34/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.1137 - mse: 0.0228 - val_loss: 0.1146 - val_mse: 0.0248\n",
      "Epoch 35/200\n",
      "808/808 [==============================] - 25s 32ms/step - loss: 0.1124 - mse: 0.0221 - val_loss: 0.1234 - val_mse: 0.0291\n",
      "Epoch 36/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1093 - mse: 0.0210 - val_loss: 0.1256 - val_mse: 0.0274\n",
      "Epoch 37/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1072 - mse: 0.0198 - val_loss: 0.1081 - val_mse: 0.0223\n",
      "Epoch 38/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1078 - mse: 0.0201 - val_loss: 0.1200 - val_mse: 0.0263\n",
      "Epoch 39/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1062 - mse: 0.0196 - val_loss: 0.1103 - val_mse: 0.0220\n",
      "Epoch 40/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1049 - mse: 0.0190 - val_loss: 0.1196 - val_mse: 0.0272\n",
      "Epoch 41/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1048 - mse: 0.0190 - val_loss: 0.1062 - val_mse: 0.0203\n",
      "Epoch 42/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1018 - mse: 0.0180 - val_loss: 0.1160 - val_mse: 0.0243\n",
      "Epoch 43/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1008 - mse: 0.0177 - val_loss: 0.1090 - val_mse: 0.0216\n",
      "Epoch 44/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.1002 - mse: 0.0175 - val_loss: 0.1102 - val_mse: 0.0229\n",
      "Epoch 45/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0988 - mse: 0.0168 - val_loss: 0.1087 - val_mse: 0.0220\n",
      "Epoch 46/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0993 - mse: 0.0171 - val_loss: 0.1143 - val_mse: 0.0251\n",
      "Epoch 47/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0969 - mse: 0.0163 - val_loss: 0.0904 - val_mse: 0.0157\n",
      "Epoch 48/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0973 - mse: 0.0165 - val_loss: 0.1040 - val_mse: 0.0204\n",
      "Epoch 49/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0959 - mse: 0.0158 - val_loss: 0.1135 - val_mse: 0.0245\n",
      "Epoch 50/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0953 - mse: 0.0158 - val_loss: 0.1252 - val_mse: 0.0286\n",
      "Epoch 51/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0943 - mse: 0.0153 - val_loss: 0.0974 - val_mse: 0.0177\n",
      "Epoch 52/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0929 - mse: 0.0149 - val_loss: 0.1014 - val_mse: 0.0198\n",
      "Epoch 53/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0922 - mse: 0.0150 - val_loss: 0.1071 - val_mse: 0.0200\n",
      "Epoch 54/200\n",
      "808/808 [==============================] - 25s 32ms/step - loss: 0.0916 - mse: 0.0146 - val_loss: 0.1125 - val_mse: 0.0232\n",
      "Epoch 55/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0899 - mse: 0.0138 - val_loss: 0.1343 - val_mse: 0.0305\n",
      "Epoch 56/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0904 - mse: 0.0142 - val_loss: 0.0981 - val_mse: 0.0175\n",
      "Epoch 57/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0887 - mse: 0.0134 - val_loss: 0.1107 - val_mse: 0.0217\n",
      "Epoch 58/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0894 - mse: 0.0138 - val_loss: 0.1062 - val_mse: 0.0203\n",
      "Epoch 59/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0883 - mse: 0.0136 - val_loss: 0.1111 - val_mse: 0.0226\n",
      "Epoch 60/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0863 - mse: 0.0128 - val_loss: 0.0995 - val_mse: 0.0177\n",
      "Epoch 61/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0866 - mse: 0.0131 - val_loss: 0.1126 - val_mse: 0.0227\n",
      "Epoch 62/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0857 - mse: 0.0128 - val_loss: 0.1119 - val_mse: 0.0233\n",
      "Epoch 63/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0853 - mse: 0.0128 - val_loss: 0.0978 - val_mse: 0.0172\n",
      "Epoch 64/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0852 - mse: 0.0125 - val_loss: 0.0961 - val_mse: 0.0171\n",
      "Epoch 65/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0858 - mse: 0.0129 - val_loss: 0.1049 - val_mse: 0.0207\n",
      "Epoch 66/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0815 - mse: 0.0115 - val_loss: 0.1066 - val_mse: 0.0209\n",
      "Epoch 67/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0833 - mse: 0.0120 - val_loss: 0.1056 - val_mse: 0.0221\n",
      "Epoch 68/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0839 - mse: 0.0121 - val_loss: 0.0947 - val_mse: 0.0167\n",
      "Epoch 69/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0843 - mse: 0.0123 - val_loss: 0.1170 - val_mse: 0.0246\n",
      "Epoch 70/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0823 - mse: 0.0118 - val_loss: 0.1001 - val_mse: 0.0168\n",
      "Epoch 71/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0827 - mse: 0.0120 - val_loss: 0.1017 - val_mse: 0.0175\n",
      "Epoch 72/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0819 - mse: 0.0117 - val_loss: 0.0915 - val_mse: 0.0165\n",
      "Epoch 73/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0806 - mse: 0.0113 - val_loss: 0.1092 - val_mse: 0.0217\n",
      "Epoch 74/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0800 - mse: 0.0111 - val_loss: 0.0948 - val_mse: 0.0166\n",
      "Epoch 75/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0802 - mse: 0.0110 - val_loss: 0.0954 - val_mse: 0.0167\n",
      "Epoch 76/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0784 - mse: 0.0106 - val_loss: 0.0883 - val_mse: 0.0157\n",
      "Epoch 77/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0796 - mse: 0.0111 - val_loss: 0.0997 - val_mse: 0.0190\n",
      "Epoch 78/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0793 - mse: 0.0111 - val_loss: 0.1090 - val_mse: 0.0214\n",
      "Epoch 79/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0782 - mse: 0.0107 - val_loss: 0.1137 - val_mse: 0.0249\n",
      "Epoch 80/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0772 - mse: 0.0104 - val_loss: 0.0929 - val_mse: 0.0153\n",
      "Epoch 81/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0772 - mse: 0.0105 - val_loss: 0.0849 - val_mse: 0.0129\n",
      "Epoch 82/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0771 - mse: 0.0103 - val_loss: 0.1036 - val_mse: 0.0195\n",
      "Epoch 83/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0763 - mse: 0.0101 - val_loss: 0.1161 - val_mse: 0.0242\n",
      "Epoch 84/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0754 - mse: 0.0100 - val_loss: 0.1041 - val_mse: 0.0219\n",
      "Epoch 85/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0763 - mse: 0.0101 - val_loss: 0.0866 - val_mse: 0.0144\n",
      "Epoch 86/200\n",
      "808/808 [==============================] - 25s 32ms/step - loss: 0.0759 - mse: 0.0101 - val_loss: 0.0985 - val_mse: 0.0173\n",
      "Epoch 87/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0752 - mse: 0.0098 - val_loss: 0.0862 - val_mse: 0.0136\n",
      "Epoch 88/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0758 - mse: 0.0100 - val_loss: 0.1063 - val_mse: 0.0200\n",
      "Epoch 89/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0747 - mse: 0.0096 - val_loss: 0.1214 - val_mse: 0.0257\n",
      "Epoch 90/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0741 - mse: 0.0096 - val_loss: 0.1114 - val_mse: 0.0231\n",
      "Epoch 91/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0749 - mse: 0.0098 - val_loss: 0.1037 - val_mse: 0.0195\n",
      "Epoch 92/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0729 - mse: 0.0091 - val_loss: 0.1273 - val_mse: 0.0274\n",
      "Epoch 93/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0731 - mse: 0.0093 - val_loss: 0.0946 - val_mse: 0.0170\n",
      "Epoch 94/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0741 - mse: 0.0097 - val_loss: 0.1145 - val_mse: 0.0234\n",
      "Epoch 95/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0724 - mse: 0.0090 - val_loss: 0.0889 - val_mse: 0.0141\n",
      "Epoch 96/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0729 - mse: 0.0094 - val_loss: 0.0919 - val_mse: 0.0147\n",
      "Epoch 97/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0718 - mse: 0.0089 - val_loss: 0.1013 - val_mse: 0.0186\n",
      "Epoch 98/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0722 - mse: 0.0090 - val_loss: 0.1041 - val_mse: 0.0200\n",
      "Epoch 99/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0717 - mse: 0.0090 - val_loss: 0.1027 - val_mse: 0.0191\n",
      "Epoch 100/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0714 - mse: 0.0088 - val_loss: 0.0964 - val_mse: 0.0170\n",
      "Epoch 101/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0721 - mse: 0.0091 - val_loss: 0.0901 - val_mse: 0.0148\n",
      "Epoch 102/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0708 - mse: 0.0090 - val_loss: 0.1053 - val_mse: 0.0184\n",
      "Epoch 103/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0700 - mse: 0.0086 - val_loss: 0.1127 - val_mse: 0.0232\n",
      "Epoch 104/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0716 - mse: 0.0090 - val_loss: 0.1096 - val_mse: 0.0212\n",
      "Epoch 105/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0699 - mse: 0.0086 - val_loss: 0.1115 - val_mse: 0.0223\n",
      "Epoch 106/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0709 - mse: 0.0089 - val_loss: 0.0907 - val_mse: 0.0152\n",
      "Epoch 107/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0702 - mse: 0.0087 - val_loss: 0.0911 - val_mse: 0.0161\n",
      "Epoch 108/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0694 - mse: 0.0083 - val_loss: 0.1084 - val_mse: 0.0202\n",
      "Epoch 109/200\n",
      "808/808 [==============================] - 25s 31ms/step - loss: 0.0694 - mse: 0.0085 - val_loss: 0.0916 - val_mse: 0.0165\n",
      "Epoch 110/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0676 - mse: 0.0079 - val_loss: 0.1210 - val_mse: 0.0265\n",
      "Epoch 111/200\n",
      "808/808 [==============================] - 26s 32ms/step - loss: 0.0690 - mse: 0.0084 - val_loss: 0.0979 - val_mse: 0.0187\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_3, test_size=0.2)\n",
    "\n",
    "model_3 = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(window_size_3, num_features)),\n",
    "    Dropout(0.3),\n",
    "    GRU(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "model_3.compile(optimizer=Adam(learning_rate=0.001),loss='mae',metrics=['mse'])\n",
    "# Train the model\n",
    "history_3 = model_3.fit(X_train,y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    epochs=200, batch_size=16,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(patience=30,monitor='val_loss',restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "764/764 [==============================] - 46s 48ms/step - loss: 0.6221 - mse: 0.5962 - val_loss: 0.5998 - val_mse: 0.5573\n",
      "Epoch 2/200\n",
      "764/764 [==============================] - 34s 45ms/step - loss: 0.6009 - mse: 0.5674 - val_loss: 0.5748 - val_mse: 0.5235\n",
      "Epoch 3/200\n",
      "764/764 [==============================] - 33s 43ms/step - loss: 0.5518 - mse: 0.5000 - val_loss: 0.5019 - val_mse: 0.4119\n",
      "Epoch 4/200\n",
      "764/764 [==============================] - 34s 44ms/step - loss: 0.4821 - mse: 0.3993 - val_loss: 0.4293 - val_mse: 0.3284\n",
      "Epoch 5/200\n",
      "764/764 [==============================] - 34s 45ms/step - loss: 0.4010 - mse: 0.2843 - val_loss: 0.3589 - val_mse: 0.2241\n",
      "Epoch 6/200\n",
      "764/764 [==============================] - 34s 44ms/step - loss: 0.3305 - mse: 0.1991 - val_loss: 0.3131 - val_mse: 0.1728\n",
      "Epoch 7/200\n",
      "764/764 [==============================] - 33s 43ms/step - loss: 0.2803 - mse: 0.1452 - val_loss: 0.2538 - val_mse: 0.1164\n",
      "Epoch 8/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.2421 - mse: 0.1093 - val_loss: 0.2222 - val_mse: 0.0926\n",
      "Epoch 9/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.2143 - mse: 0.0860 - val_loss: 0.1972 - val_mse: 0.0755\n",
      "Epoch 10/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1875 - mse: 0.0666 - val_loss: 0.1683 - val_mse: 0.0526\n",
      "Epoch 11/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1727 - mse: 0.0546 - val_loss: 0.1829 - val_mse: 0.0631\n",
      "Epoch 12/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1571 - mse: 0.0450 - val_loss: 0.1726 - val_mse: 0.0571\n",
      "Epoch 13/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1522 - mse: 0.0414 - val_loss: 0.1380 - val_mse: 0.0353\n",
      "Epoch 14/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1415 - mse: 0.0354 - val_loss: 0.1278 - val_mse: 0.0319\n",
      "Epoch 15/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1339 - mse: 0.0317 - val_loss: 0.1147 - val_mse: 0.0259\n",
      "Epoch 16/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1275 - mse: 0.0291 - val_loss: 0.1111 - val_mse: 0.0230\n",
      "Epoch 17/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1257 - mse: 0.0278 - val_loss: 0.1370 - val_mse: 0.0351\n",
      "Epoch 18/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1176 - mse: 0.0242 - val_loss: 0.1056 - val_mse: 0.0211\n",
      "Epoch 19/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1163 - mse: 0.0237 - val_loss: 0.1161 - val_mse: 0.0231\n",
      "Epoch 20/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1120 - mse: 0.0220 - val_loss: 0.1297 - val_mse: 0.0304\n",
      "Epoch 21/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1106 - mse: 0.0215 - val_loss: 0.1033 - val_mse: 0.0195\n",
      "Epoch 22/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.1052 - mse: 0.0195 - val_loss: 0.0969 - val_mse: 0.0167\n",
      "Epoch 23/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.1028 - mse: 0.0184 - val_loss: 0.0896 - val_mse: 0.0151\n",
      "Epoch 24/200\n",
      "764/764 [==============================] - 31s 41ms/step - loss: 0.1021 - mse: 0.0182 - val_loss: 0.0886 - val_mse: 0.0141\n",
      "Epoch 25/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0993 - mse: 0.0171 - val_loss: 0.1163 - val_mse: 0.0235\n",
      "Epoch 26/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0960 - mse: 0.0162 - val_loss: 0.1012 - val_mse: 0.0177\n",
      "Epoch 27/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0952 - mse: 0.0157 - val_loss: 0.1067 - val_mse: 0.0199\n",
      "Epoch 28/200\n",
      "764/764 [==============================] - 33s 43ms/step - loss: 0.0934 - mse: 0.0152 - val_loss: 0.1108 - val_mse: 0.0212\n",
      "Epoch 29/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0924 - mse: 0.0150 - val_loss: 0.0843 - val_mse: 0.0136\n",
      "Epoch 30/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0882 - mse: 0.0136 - val_loss: 0.0883 - val_mse: 0.0139\n",
      "Epoch 31/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0874 - mse: 0.0134 - val_loss: 0.0831 - val_mse: 0.0134\n",
      "Epoch 32/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0867 - mse: 0.0130 - val_loss: 0.0962 - val_mse: 0.0162\n",
      "Epoch 33/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0854 - mse: 0.0126 - val_loss: 0.0973 - val_mse: 0.0169\n",
      "Epoch 34/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0855 - mse: 0.0126 - val_loss: 0.0934 - val_mse: 0.0146\n",
      "Epoch 35/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0827 - mse: 0.0122 - val_loss: 0.1208 - val_mse: 0.0246\n",
      "Epoch 36/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0834 - mse: 0.0121 - val_loss: 0.1165 - val_mse: 0.0232\n",
      "Epoch 37/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0799 - mse: 0.0111 - val_loss: 0.1024 - val_mse: 0.0176\n",
      "Epoch 38/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0805 - mse: 0.0113 - val_loss: 0.0935 - val_mse: 0.0150\n",
      "Epoch 39/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0787 - mse: 0.0108 - val_loss: 0.0957 - val_mse: 0.0178\n",
      "Epoch 40/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0784 - mse: 0.0108 - val_loss: 0.0895 - val_mse: 0.0139\n",
      "Epoch 41/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0781 - mse: 0.0106 - val_loss: 0.0828 - val_mse: 0.0124\n",
      "Epoch 42/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0751 - mse: 0.0098 - val_loss: 0.1066 - val_mse: 0.0197\n",
      "Epoch 43/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0762 - mse: 0.0100 - val_loss: 0.1055 - val_mse: 0.0182\n",
      "Epoch 44/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0746 - mse: 0.0098 - val_loss: 0.1046 - val_mse: 0.0173\n",
      "Epoch 45/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0726 - mse: 0.0092 - val_loss: 0.0928 - val_mse: 0.0145\n",
      "Epoch 46/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0728 - mse: 0.0092 - val_loss: 0.0928 - val_mse: 0.0144\n",
      "Epoch 47/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0711 - mse: 0.0090 - val_loss: 0.0838 - val_mse: 0.0114\n",
      "Epoch 48/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0724 - mse: 0.0091 - val_loss: 0.0898 - val_mse: 0.0136\n",
      "Epoch 49/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0702 - mse: 0.0088 - val_loss: 0.0958 - val_mse: 0.0148\n",
      "Epoch 50/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0711 - mse: 0.0089 - val_loss: 0.0905 - val_mse: 0.0133\n",
      "Epoch 51/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0689 - mse: 0.0082 - val_loss: 0.0918 - val_mse: 0.0157\n",
      "Epoch 52/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0680 - mse: 0.0081 - val_loss: 0.0984 - val_mse: 0.0164\n",
      "Epoch 53/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0679 - mse: 0.0081 - val_loss: 0.0805 - val_mse: 0.0119\n",
      "Epoch 54/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0679 - mse: 0.0081 - val_loss: 0.0854 - val_mse: 0.0136\n",
      "Epoch 55/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0661 - mse: 0.0077 - val_loss: 0.0694 - val_mse: 0.0097\n",
      "Epoch 56/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0663 - mse: 0.0077 - val_loss: 0.1035 - val_mse: 0.0181\n",
      "Epoch 57/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0652 - mse: 0.0075 - val_loss: 0.0966 - val_mse: 0.0160\n",
      "Epoch 58/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0655 - mse: 0.0075 - val_loss: 0.0704 - val_mse: 0.0092\n",
      "Epoch 59/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0655 - mse: 0.0077 - val_loss: 0.0843 - val_mse: 0.0124\n",
      "Epoch 60/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0642 - mse: 0.0074 - val_loss: 0.0925 - val_mse: 0.0137\n",
      "Epoch 61/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0624 - mse: 0.0067 - val_loss: 0.0986 - val_mse: 0.0150\n",
      "Epoch 62/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0627 - mse: 0.0068 - val_loss: 0.1004 - val_mse: 0.0168\n",
      "Epoch 63/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0622 - mse: 0.0069 - val_loss: 0.0865 - val_mse: 0.0131\n",
      "Epoch 64/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0617 - mse: 0.0067 - val_loss: 0.0765 - val_mse: 0.0102\n",
      "Epoch 65/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0606 - mse: 0.0064 - val_loss: 0.0804 - val_mse: 0.0115\n",
      "Epoch 66/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0604 - mse: 0.0064 - val_loss: 0.0779 - val_mse: 0.0106\n",
      "Epoch 67/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0600 - mse: 0.0063 - val_loss: 0.0908 - val_mse: 0.0148\n",
      "Epoch 68/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0605 - mse: 0.0067 - val_loss: 0.0965 - val_mse: 0.0155\n",
      "Epoch 69/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0602 - mse: 0.0064 - val_loss: 0.1035 - val_mse: 0.0180\n",
      "Epoch 70/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0600 - mse: 0.0064 - val_loss: 0.1147 - val_mse: 0.0205\n",
      "Epoch 71/200\n",
      "764/764 [==============================] - 31s 41ms/step - loss: 0.0593 - mse: 0.0063 - val_loss: 0.0905 - val_mse: 0.0142\n",
      "Epoch 72/200\n",
      "764/764 [==============================] - 31s 41ms/step - loss: 0.0580 - mse: 0.0059 - val_loss: 0.0997 - val_mse: 0.0169\n",
      "Epoch 73/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0583 - mse: 0.0060 - val_loss: 0.0942 - val_mse: 0.0158\n",
      "Epoch 74/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0582 - mse: 0.0061 - val_loss: 0.0938 - val_mse: 0.0156\n",
      "Epoch 75/200\n",
      "764/764 [==============================] - 31s 41ms/step - loss: 0.0577 - mse: 0.0060 - val_loss: 0.0946 - val_mse: 0.0146\n",
      "Epoch 76/200\n",
      "764/764 [==============================] - 31s 41ms/step - loss: 0.0567 - mse: 0.0057 - val_loss: 0.0912 - val_mse: 0.0133\n",
      "Epoch 77/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0555 - mse: 0.0054 - val_loss: 0.0866 - val_mse: 0.0134\n",
      "Epoch 78/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0571 - mse: 0.0058 - val_loss: 0.0886 - val_mse: 0.0127\n",
      "Epoch 79/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0567 - mse: 0.0056 - val_loss: 0.0899 - val_mse: 0.0137\n",
      "Epoch 80/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0561 - mse: 0.0055 - val_loss: 0.1002 - val_mse: 0.0157\n",
      "Epoch 81/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0552 - mse: 0.0054 - val_loss: 0.0829 - val_mse: 0.0121\n",
      "Epoch 82/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0543 - mse: 0.0052 - val_loss: 0.1039 - val_mse: 0.0176\n",
      "Epoch 83/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0557 - mse: 0.0055 - val_loss: 0.0942 - val_mse: 0.0148\n",
      "Epoch 84/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0553 - mse: 0.0054 - val_loss: 0.1008 - val_mse: 0.0168\n",
      "Epoch 85/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0539 - mse: 0.0051 - val_loss: 0.0668 - val_mse: 0.0082\n",
      "Epoch 86/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0544 - mse: 0.0052 - val_loss: 0.0993 - val_mse: 0.0157\n",
      "Epoch 87/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0539 - mse: 0.0052 - val_loss: 0.0925 - val_mse: 0.0143\n",
      "Epoch 88/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0551 - mse: 0.0058 - val_loss: 0.1021 - val_mse: 0.0174\n",
      "Epoch 89/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0538 - mse: 0.0051 - val_loss: 0.0883 - val_mse: 0.0130\n",
      "Epoch 90/200\n",
      "764/764 [==============================] - 31s 41ms/step - loss: 0.0523 - mse: 0.0048 - val_loss: 0.0773 - val_mse: 0.0100\n",
      "Epoch 91/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0526 - mse: 0.0050 - val_loss: 0.0980 - val_mse: 0.0153\n",
      "Epoch 92/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0524 - mse: 0.0049 - val_loss: 0.1052 - val_mse: 0.0188\n",
      "Epoch 93/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0519 - mse: 0.0048 - val_loss: 0.0976 - val_mse: 0.0149\n",
      "Epoch 94/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0519 - mse: 0.0048 - val_loss: 0.0882 - val_mse: 0.0129\n",
      "Epoch 95/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0535 - mse: 0.0053 - val_loss: 0.1000 - val_mse: 0.0172\n",
      "Epoch 96/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0514 - mse: 0.0048 - val_loss: 0.0880 - val_mse: 0.0130\n",
      "Epoch 97/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0520 - mse: 0.0048 - val_loss: 0.0784 - val_mse: 0.0101\n",
      "Epoch 98/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0508 - mse: 0.0045 - val_loss: 0.0951 - val_mse: 0.0153\n",
      "Epoch 99/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0508 - mse: 0.0046 - val_loss: 0.0850 - val_mse: 0.0129\n",
      "Epoch 100/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0500 - mse: 0.0045 - val_loss: 0.0895 - val_mse: 0.0153\n",
      "Epoch 101/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0509 - mse: 0.0049 - val_loss: 0.0819 - val_mse: 0.0125\n",
      "Epoch 102/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0504 - mse: 0.0046 - val_loss: 0.0819 - val_mse: 0.0106\n",
      "Epoch 103/200\n",
      "764/764 [==============================] - 32s 41ms/step - loss: 0.0605 - mse: 0.0102 - val_loss: 0.0759 - val_mse: 0.0090\n",
      "Epoch 104/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0484 - mse: 0.0042 - val_loss: 0.0891 - val_mse: 0.0132\n",
      "Epoch 105/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0492 - mse: 0.0042 - val_loss: 0.0842 - val_mse: 0.0129\n",
      "Epoch 106/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0488 - mse: 0.0042 - val_loss: 0.0759 - val_mse: 0.0100\n",
      "Epoch 107/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0484 - mse: 0.0041 - val_loss: 0.0767 - val_mse: 0.0096\n",
      "Epoch 108/200\n",
      "764/764 [==============================] - 33s 43ms/step - loss: 0.0476 - mse: 0.0040 - val_loss: 0.1032 - val_mse: 0.0168\n",
      "Epoch 109/200\n",
      "764/764 [==============================] - 33s 43ms/step - loss: 0.0492 - mse: 0.0043 - val_loss: 0.0861 - val_mse: 0.0114\n",
      "Epoch 110/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0488 - mse: 0.0045 - val_loss: 0.0838 - val_mse: 0.0124\n",
      "Epoch 111/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0479 - mse: 0.0040 - val_loss: 0.1004 - val_mse: 0.0164\n",
      "Epoch 112/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0482 - mse: 0.0042 - val_loss: 0.0841 - val_mse: 0.0118\n",
      "Epoch 113/200\n",
      "764/764 [==============================] - 33s 43ms/step - loss: 0.0487 - mse: 0.0042 - val_loss: 0.0921 - val_mse: 0.0139\n",
      "Epoch 114/200\n",
      "764/764 [==============================] - 32s 42ms/step - loss: 0.0477 - mse: 0.0041 - val_loss: 0.0934 - val_mse: 0.0147\n",
      "Epoch 115/200\n",
      "764/764 [==============================] - 33s 43ms/step - loss: 0.0472 - mse: 0.0040 - val_loss: 0.1049 - val_mse: 0.0185\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_4, test_size=0.2)\n",
    "\n",
    "model_4 = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(window_size_4, num_features)),\n",
    "    Dropout(0.3),\n",
    "    GRU(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "model_4.compile(optimizer=Adam(learning_rate=0.001),loss='mae',metrics=['mse'])\n",
    "# Train the model\n",
    "history_4 = model_4.fit(X_train,y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    epochs=200, batch_size=16,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(patience=30,monitor='val_loss',restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\.conda\\envs\\mlEnv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:979: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(self.var_), copy=False, constant_mask=constant_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "721/721 [==============================] - 49s 57ms/step - loss: 0.6252 - mse: 0.6081 - val_loss: 0.6076 - val_mse: 0.5716\n",
      "Epoch 2/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.5958 - mse: 0.5628 - val_loss: 0.5348 - val_mse: 0.4833\n",
      "Epoch 3/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.5268 - mse: 0.4617 - val_loss: 0.5430 - val_mse: 0.4685\n",
      "Epoch 4/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.4326 - mse: 0.3252 - val_loss: 0.3579 - val_mse: 0.2358\n",
      "Epoch 5/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.3516 - mse: 0.2240 - val_loss: 0.3279 - val_mse: 0.1805\n",
      "Epoch 6/200\n",
      "721/721 [==============================] - 39s 55ms/step - loss: 0.2902 - mse: 0.1563 - val_loss: 0.2565 - val_mse: 0.1334\n",
      "Epoch 7/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.2352 - mse: 0.1050 - val_loss: 0.2095 - val_mse: 0.0806\n",
      "Epoch 8/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.2070 - mse: 0.0811 - val_loss: 0.1756 - val_mse: 0.0634\n",
      "Epoch 9/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1835 - mse: 0.0631 - val_loss: 0.1994 - val_mse: 0.0693\n",
      "Epoch 10/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1619 - mse: 0.0495 - val_loss: 0.1278 - val_mse: 0.0316\n",
      "Epoch 11/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1538 - mse: 0.0438 - val_loss: 0.1371 - val_mse: 0.0350\n",
      "Epoch 12/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1443 - mse: 0.0378 - val_loss: 0.1158 - val_mse: 0.0259\n",
      "Epoch 13/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1335 - mse: 0.0332 - val_loss: 0.1059 - val_mse: 0.0210\n",
      "Epoch 14/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1297 - mse: 0.0307 - val_loss: 0.1157 - val_mse: 0.0231\n",
      "Epoch 15/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.1204 - mse: 0.0259 - val_loss: 0.1175 - val_mse: 0.0226\n",
      "Epoch 16/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.1161 - mse: 0.0243 - val_loss: 0.1319 - val_mse: 0.0291\n",
      "Epoch 17/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1117 - mse: 0.0220 - val_loss: 0.0893 - val_mse: 0.0142\n",
      "Epoch 18/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.1098 - mse: 0.0214 - val_loss: 0.0971 - val_mse: 0.0167\n",
      "Epoch 19/200\n",
      "721/721 [==============================] - 39s 55ms/step - loss: 0.1046 - mse: 0.0193 - val_loss: 0.1326 - val_mse: 0.0275\n",
      "Epoch 20/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.1001 - mse: 0.0177 - val_loss: 0.0762 - val_mse: 0.0102\n",
      "Epoch 21/200\n",
      "721/721 [==============================] - 41s 57ms/step - loss: 0.0983 - mse: 0.0170 - val_loss: 0.0867 - val_mse: 0.0128\n",
      "Epoch 22/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0962 - mse: 0.0166 - val_loss: 0.0902 - val_mse: 0.0146\n",
      "Epoch 23/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0954 - mse: 0.0163 - val_loss: 0.0729 - val_mse: 0.0095\n",
      "Epoch 24/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0923 - mse: 0.0152 - val_loss: 0.0962 - val_mse: 0.0160\n",
      "Epoch 25/200\n",
      "721/721 [==============================] - 39s 55ms/step - loss: 0.0887 - mse: 0.0139 - val_loss: 0.1011 - val_mse: 0.0168\n",
      "Epoch 26/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.0888 - mse: 0.0140 - val_loss: 0.1065 - val_mse: 0.0192\n",
      "Epoch 27/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.0864 - mse: 0.0133 - val_loss: 0.0850 - val_mse: 0.0125\n",
      "Epoch 28/200\n",
      "721/721 [==============================] - 44s 62ms/step - loss: 0.0852 - mse: 0.0128 - val_loss: 0.0815 - val_mse: 0.0121\n",
      "Epoch 29/200\n",
      "721/721 [==============================] - 46s 64ms/step - loss: 0.0835 - mse: 0.0123 - val_loss: 0.1012 - val_mse: 0.0173\n",
      "Epoch 30/200\n",
      "721/721 [==============================] - 45s 63ms/step - loss: 0.0829 - mse: 0.0120 - val_loss: 0.0939 - val_mse: 0.0142\n",
      "Epoch 31/200\n",
      "721/721 [==============================] - 46s 64ms/step - loss: 0.0809 - mse: 0.0118 - val_loss: 0.0804 - val_mse: 0.0112\n",
      "Epoch 32/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0782 - mse: 0.0109 - val_loss: 0.0820 - val_mse: 0.0120\n",
      "Epoch 33/200\n",
      "721/721 [==============================] - 39s 55ms/step - loss: 0.0778 - mse: 0.0107 - val_loss: 0.0991 - val_mse: 0.0172\n",
      "Epoch 34/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0766 - mse: 0.0105 - val_loss: 0.0793 - val_mse: 0.0116\n",
      "Epoch 35/200\n",
      "721/721 [==============================] - 39s 55ms/step - loss: 0.0743 - mse: 0.0098 - val_loss: 0.0707 - val_mse: 0.0089\n",
      "Epoch 36/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0740 - mse: 0.0098 - val_loss: 0.0859 - val_mse: 0.0122\n",
      "Epoch 37/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0727 - mse: 0.0094 - val_loss: 0.0790 - val_mse: 0.0103\n",
      "Epoch 38/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0710 - mse: 0.0089 - val_loss: 0.0686 - val_mse: 0.0084\n",
      "Epoch 39/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0695 - mse: 0.0086 - val_loss: 0.0660 - val_mse: 0.0071\n",
      "Epoch 40/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0710 - mse: 0.0090 - val_loss: 0.0928 - val_mse: 0.0136\n",
      "Epoch 41/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0703 - mse: 0.0088 - val_loss: 0.0684 - val_mse: 0.0086\n",
      "Epoch 42/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0679 - mse: 0.0083 - val_loss: 0.1134 - val_mse: 0.0206\n",
      "Epoch 43/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0668 - mse: 0.0079 - val_loss: 0.0838 - val_mse: 0.0126\n",
      "Epoch 44/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0656 - mse: 0.0076 - val_loss: 0.0645 - val_mse: 0.0075\n",
      "Epoch 45/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0648 - mse: 0.0076 - val_loss: 0.0809 - val_mse: 0.0115\n",
      "Epoch 46/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0643 - mse: 0.0072 - val_loss: 0.0854 - val_mse: 0.0120\n",
      "Epoch 47/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0642 - mse: 0.0074 - val_loss: 0.0738 - val_mse: 0.0090\n",
      "Epoch 48/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0621 - mse: 0.0069 - val_loss: 0.0761 - val_mse: 0.0096\n",
      "Epoch 49/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0639 - mse: 0.0074 - val_loss: 0.0959 - val_mse: 0.0150\n",
      "Epoch 50/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0634 - mse: 0.0072 - val_loss: 0.0876 - val_mse: 0.0127\n",
      "Epoch 51/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0614 - mse: 0.0067 - val_loss: 0.0698 - val_mse: 0.0085\n",
      "Epoch 52/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0598 - mse: 0.0064 - val_loss: 0.0830 - val_mse: 0.0116\n",
      "Epoch 53/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0596 - mse: 0.0064 - val_loss: 0.0686 - val_mse: 0.0080\n",
      "Epoch 54/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0603 - mse: 0.0066 - val_loss: 0.1004 - val_mse: 0.0168\n",
      "Epoch 55/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0578 - mse: 0.0060 - val_loss: 0.0934 - val_mse: 0.0147\n",
      "Epoch 56/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0581 - mse: 0.0061 - val_loss: 0.0819 - val_mse: 0.0102\n",
      "Epoch 57/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0590 - mse: 0.0063 - val_loss: 0.0855 - val_mse: 0.0122\n",
      "Epoch 58/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0576 - mse: 0.0061 - val_loss: 0.0917 - val_mse: 0.0128\n",
      "Epoch 59/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0567 - mse: 0.0058 - val_loss: 0.0877 - val_mse: 0.0122\n",
      "Epoch 60/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0554 - mse: 0.0055 - val_loss: 0.0651 - val_mse: 0.0070\n",
      "Epoch 61/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0553 - mse: 0.0055 - val_loss: 0.0901 - val_mse: 0.0135\n",
      "Epoch 62/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0552 - mse: 0.0055 - val_loss: 0.0836 - val_mse: 0.0120\n",
      "Epoch 63/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0552 - mse: 0.0055 - val_loss: 0.0791 - val_mse: 0.0102\n",
      "Epoch 64/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0552 - mse: 0.0055 - val_loss: 0.0758 - val_mse: 0.0101\n",
      "Epoch 65/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0541 - mse: 0.0053 - val_loss: 0.0942 - val_mse: 0.0137\n",
      "Epoch 66/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0525 - mse: 0.0051 - val_loss: 0.0732 - val_mse: 0.0085\n",
      "Epoch 67/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0528 - mse: 0.0051 - val_loss: 0.0601 - val_mse: 0.0071\n",
      "Epoch 68/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0532 - mse: 0.0051 - val_loss: 0.0568 - val_mse: 0.0059\n",
      "Epoch 69/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0515 - mse: 0.0047 - val_loss: 0.0772 - val_mse: 0.0095\n",
      "Epoch 70/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0509 - mse: 0.0048 - val_loss: 0.0676 - val_mse: 0.0080\n",
      "Epoch 71/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0511 - mse: 0.0048 - val_loss: 0.0747 - val_mse: 0.0088\n",
      "Epoch 72/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0504 - mse: 0.0046 - val_loss: 0.0713 - val_mse: 0.0089\n",
      "Epoch 73/200\n",
      "721/721 [==============================] - 39s 55ms/step - loss: 0.0497 - mse: 0.0045 - val_loss: 0.0647 - val_mse: 0.0072\n",
      "Epoch 74/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0505 - mse: 0.0045 - val_loss: 0.0766 - val_mse: 0.0091\n",
      "Epoch 75/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0500 - mse: 0.0046 - val_loss: 0.0898 - val_mse: 0.0130\n",
      "Epoch 76/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0481 - mse: 0.0042 - val_loss: 0.0701 - val_mse: 0.0095\n",
      "Epoch 77/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0490 - mse: 0.0044 - val_loss: 0.0753 - val_mse: 0.0087\n",
      "Epoch 78/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0483 - mse: 0.0045 - val_loss: 0.0929 - val_mse: 0.0132\n",
      "Epoch 79/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0476 - mse: 0.0041 - val_loss: 0.0840 - val_mse: 0.0119\n",
      "Epoch 80/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0480 - mse: 0.0042 - val_loss: 0.0860 - val_mse: 0.0129\n",
      "Epoch 81/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0476 - mse: 0.0041 - val_loss: 0.0761 - val_mse: 0.0093\n",
      "Epoch 82/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0472 - mse: 0.0041 - val_loss: 0.0891 - val_mse: 0.0126\n",
      "Epoch 83/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0452 - mse: 0.0039 - val_loss: 0.0869 - val_mse: 0.0120\n",
      "Epoch 84/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0463 - mse: 0.0039 - val_loss: 0.0847 - val_mse: 0.0114\n",
      "Epoch 85/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0461 - mse: 0.0039 - val_loss: 0.0718 - val_mse: 0.0088\n",
      "Epoch 86/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0456 - mse: 0.0039 - val_loss: 0.0997 - val_mse: 0.0155\n",
      "Epoch 87/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0451 - mse: 0.0038 - val_loss: 0.0703 - val_mse: 0.0077\n",
      "Epoch 88/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0451 - mse: 0.0038 - val_loss: 0.0674 - val_mse: 0.0084\n",
      "Epoch 89/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0445 - mse: 0.0037 - val_loss: 0.0687 - val_mse: 0.0077\n",
      "Epoch 90/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0446 - mse: 0.0037 - val_loss: 0.0831 - val_mse: 0.0114\n",
      "Epoch 91/200\n",
      "721/721 [==============================] - 39s 54ms/step - loss: 0.0450 - mse: 0.0038 - val_loss: 0.0786 - val_mse: 0.0099\n",
      "Epoch 92/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0442 - mse: 0.0037 - val_loss: 0.0756 - val_mse: 0.0095\n",
      "Epoch 93/200\n",
      "721/721 [==============================] - 40s 55ms/step - loss: 0.0441 - mse: 0.0035 - val_loss: 0.0826 - val_mse: 0.0111\n",
      "Epoch 94/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0434 - mse: 0.0035 - val_loss: 0.0568 - val_mse: 0.0054\n",
      "Epoch 95/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0432 - mse: 0.0035 - val_loss: 0.0759 - val_mse: 0.0088\n",
      "Epoch 96/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0432 - mse: 0.0034 - val_loss: 0.0694 - val_mse: 0.0078\n",
      "Epoch 97/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0420 - mse: 0.0032 - val_loss: 0.0683 - val_mse: 0.0083\n",
      "Epoch 98/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0430 - mse: 0.0037 - val_loss: 0.0755 - val_mse: 0.0083\n",
      "Epoch 99/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0419 - mse: 0.0033 - val_loss: 0.0722 - val_mse: 0.0084\n",
      "Epoch 100/200\n",
      "721/721 [==============================] - 41s 57ms/step - loss: 0.0404 - mse: 0.0031 - val_loss: 0.0776 - val_mse: 0.0096\n",
      "Epoch 101/200\n",
      "721/721 [==============================] - 41s 57ms/step - loss: 0.0423 - mse: 0.0033 - val_loss: 0.0724 - val_mse: 0.0084\n",
      "Epoch 102/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0418 - mse: 0.0032 - val_loss: 0.0802 - val_mse: 0.0101\n",
      "Epoch 103/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0409 - mse: 0.0033 - val_loss: 0.0793 - val_mse: 0.0099\n",
      "Epoch 104/200\n",
      "721/721 [==============================] - 41s 57ms/step - loss: 0.0411 - mse: 0.0032 - val_loss: 0.0758 - val_mse: 0.0099\n",
      "Epoch 105/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0407 - mse: 0.0030 - val_loss: 0.0582 - val_mse: 0.0060\n",
      "Epoch 106/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0401 - mse: 0.0030 - val_loss: 0.0681 - val_mse: 0.0084\n",
      "Epoch 107/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0403 - mse: 0.0030 - val_loss: 0.0803 - val_mse: 0.0099\n",
      "Epoch 108/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0407 - mse: 0.0032 - val_loss: 0.0753 - val_mse: 0.0087\n",
      "Epoch 109/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0396 - mse: 0.0030 - val_loss: 0.0880 - val_mse: 0.0129\n",
      "Epoch 110/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0397 - mse: 0.0030 - val_loss: 0.0759 - val_mse: 0.0105\n",
      "Epoch 111/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0404 - mse: 0.0032 - val_loss: 0.0797 - val_mse: 0.0109\n",
      "Epoch 112/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0404 - mse: 0.0031 - val_loss: 0.0722 - val_mse: 0.0087\n",
      "Epoch 113/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0380 - mse: 0.0027 - val_loss: 0.0739 - val_mse: 0.0105\n",
      "Epoch 114/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0389 - mse: 0.0028 - val_loss: 0.0794 - val_mse: 0.0102\n",
      "Epoch 115/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0387 - mse: 0.0029 - val_loss: 0.0832 - val_mse: 0.0125\n",
      "Epoch 116/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0386 - mse: 0.0028 - val_loss: 0.0716 - val_mse: 0.0088\n",
      "Epoch 117/200\n",
      "721/721 [==============================] - 41s 57ms/step - loss: 0.0389 - mse: 0.0029 - val_loss: 0.0802 - val_mse: 0.0106\n",
      "Epoch 118/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0388 - mse: 0.0029 - val_loss: 0.0725 - val_mse: 0.0081\n",
      "Epoch 119/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0389 - mse: 0.0030 - val_loss: 0.0773 - val_mse: 0.0102\n",
      "Epoch 120/200\n",
      "721/721 [==============================] - 41s 57ms/step - loss: 0.0373 - mse: 0.0027 - val_loss: 0.0729 - val_mse: 0.0090\n",
      "Epoch 121/200\n",
      "721/721 [==============================] - 41s 56ms/step - loss: 0.0378 - mse: 0.0027 - val_loss: 0.0599 - val_mse: 0.0070\n",
      "Epoch 122/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0384 - mse: 0.0028 - val_loss: 0.0700 - val_mse: 0.0087\n",
      "Epoch 123/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0371 - mse: 0.0026 - val_loss: 0.0787 - val_mse: 0.0103\n",
      "Epoch 124/200\n",
      "721/721 [==============================] - 40s 56ms/step - loss: 0.0369 - mse: 0.0026 - val_loss: 0.0657 - val_mse: 0.0075\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_5, test_size=0.2)\n",
    "\n",
    "model_5 = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(window_size_5, num_features)),\n",
    "    Dropout(0.3),\n",
    "    GRU(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "model_5.compile(optimizer=Adam(learning_rate=0.001),loss='mae',metrics=['mse'])\n",
    "# Train the model\n",
    "history_5 = model_5.fit(X_train,y_train,\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    epochs=200, batch_size=16,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(patience=30,monitor='val_loss',restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "678/678 [==============================] - 55s 70ms/step - loss: 0.6351 - mse: 0.6253 - val_loss: 0.6181 - val_mse: 0.6110\n",
      "Epoch 2/200\n",
      "678/678 [==============================] - 46s 67ms/step - loss: 0.6066 - mse: 0.5840 - val_loss: 0.5776 - val_mse: 0.5418\n",
      "Epoch 3/200\n",
      "678/678 [==============================] - 46s 69ms/step - loss: 0.5611 - mse: 0.5219 - val_loss: 0.4953 - val_mse: 0.4278\n",
      "Epoch 4/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.4688 - mse: 0.3911 - val_loss: 0.3992 - val_mse: 0.2964\n",
      "Epoch 5/200\n",
      "678/678 [==============================] - 46s 67ms/step - loss: 0.3638 - mse: 0.2522 - val_loss: 0.2933 - val_mse: 0.1680\n",
      "Epoch 6/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.2883 - mse: 0.1635 - val_loss: 0.2268 - val_mse: 0.1037\n",
      "Epoch 7/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.2269 - mse: 0.1006 - val_loss: 0.1865 - val_mse: 0.0705\n",
      "Epoch 8/200\n",
      "678/678 [==============================] - 46s 67ms/step - loss: 0.1920 - mse: 0.0705 - val_loss: 0.1474 - val_mse: 0.0399\n",
      "Epoch 9/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.1642 - mse: 0.0504 - val_loss: 0.1335 - val_mse: 0.0325\n",
      "Epoch 10/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.1501 - mse: 0.0422 - val_loss: 0.1090 - val_mse: 0.0225\n",
      "Epoch 11/200\n",
      "678/678 [==============================] - 46s 68ms/step - loss: 0.1354 - mse: 0.0334 - val_loss: 0.1227 - val_mse: 0.0259\n",
      "Epoch 12/200\n",
      "678/678 [==============================] - 46s 67ms/step - loss: 0.1280 - mse: 0.0301 - val_loss: 0.1078 - val_mse: 0.0209\n",
      "Epoch 13/200\n",
      "678/678 [==============================] - 46s 67ms/step - loss: 0.1218 - mse: 0.0276 - val_loss: 0.1083 - val_mse: 0.0204\n",
      "Epoch 14/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.1140 - mse: 0.0239 - val_loss: 0.1009 - val_mse: 0.0173\n",
      "Epoch 15/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.1094 - mse: 0.0218 - val_loss: 0.1030 - val_mse: 0.0188\n",
      "Epoch 16/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.1039 - mse: 0.0197 - val_loss: 0.0864 - val_mse: 0.0127\n",
      "Epoch 17/200\n",
      "678/678 [==============================] - 45s 66ms/step - loss: 0.1030 - mse: 0.0188 - val_loss: 0.0938 - val_mse: 0.0142\n",
      "Epoch 18/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.0995 - mse: 0.0178 - val_loss: 0.1135 - val_mse: 0.0202\n",
      "Epoch 19/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.0941 - mse: 0.0163 - val_loss: 0.0960 - val_mse: 0.0152\n",
      "Epoch 20/200\n",
      "678/678 [==============================] - 45s 67ms/step - loss: 0.0924 - mse: 0.0155 - val_loss: 0.0739 - val_mse: 0.0098\n",
      "Epoch 21/200\n",
      "678/678 [==============================] - 48s 70ms/step - loss: 0.0898 - mse: 0.0145 - val_loss: 0.0904 - val_mse: 0.0136\n",
      "Epoch 22/200\n",
      "678/678 [==============================] - 55s 81ms/step - loss: 0.0857 - mse: 0.0133 - val_loss: 0.1247 - val_mse: 0.0245\n",
      "Epoch 23/200\n",
      "678/678 [==============================] - 56s 83ms/step - loss: 0.0878 - mse: 0.0138 - val_loss: 0.1082 - val_mse: 0.0183\n",
      "Epoch 24/200\n",
      "678/678 [==============================] - 54s 79ms/step - loss: 0.0832 - mse: 0.0124 - val_loss: 0.0732 - val_mse: 0.0101\n",
      "Epoch 25/200\n",
      "678/678 [==============================] - 53s 78ms/step - loss: 0.0819 - mse: 0.0121 - val_loss: 0.0768 - val_mse: 0.0104\n",
      "Epoch 26/200\n",
      "678/678 [==============================] - 54s 80ms/step - loss: 0.0791 - mse: 0.0117 - val_loss: 0.0859 - val_mse: 0.0129\n",
      "Epoch 27/200\n",
      "678/678 [==============================] - 53s 78ms/step - loss: 0.0776 - mse: 0.0107 - val_loss: 0.0680 - val_mse: 0.0077\n",
      "Epoch 28/200\n",
      "678/678 [==============================] - 56s 82ms/step - loss: 0.0762 - mse: 0.0106 - val_loss: 0.0778 - val_mse: 0.0095\n",
      "Epoch 29/200\n",
      "678/678 [==============================] - 53s 78ms/step - loss: 0.0748 - mse: 0.0101 - val_loss: 0.0900 - val_mse: 0.0132\n",
      "Epoch 30/200\n",
      "678/678 [==============================] - 55s 82ms/step - loss: 0.0732 - mse: 0.0097 - val_loss: 0.0849 - val_mse: 0.0125\n",
      "Epoch 31/200\n",
      "678/678 [==============================] - 59s 87ms/step - loss: 0.0712 - mse: 0.0091 - val_loss: 0.0626 - val_mse: 0.0068\n",
      "Epoch 32/200\n",
      "678/678 [==============================] - 56s 82ms/step - loss: 0.0714 - mse: 0.0092 - val_loss: 0.0801 - val_mse: 0.0092\n",
      "Epoch 33/200\n",
      "678/678 [==============================] - 55s 81ms/step - loss: 0.0685 - mse: 0.0086 - val_loss: 0.0737 - val_mse: 0.0099\n",
      "Epoch 34/200\n",
      "678/678 [==============================] - 57s 84ms/step - loss: 0.0685 - mse: 0.0086 - val_loss: 0.0610 - val_mse: 0.0067\n",
      "Epoch 35/200\n",
      "678/678 [==============================] - 55s 82ms/step - loss: 0.0674 - mse: 0.0082 - val_loss: 0.0633 - val_mse: 0.0067\n",
      "Epoch 36/200\n",
      "678/678 [==============================] - 55s 82ms/step - loss: 0.0660 - mse: 0.0080 - val_loss: 0.0783 - val_mse: 0.0102\n",
      "Epoch 37/200\n",
      "678/678 [==============================] - 51s 76ms/step - loss: 0.0644 - mse: 0.0074 - val_loss: 0.0718 - val_mse: 0.0095\n",
      "Epoch 38/200\n",
      "678/678 [==============================] - 52s 76ms/step - loss: 0.0643 - mse: 0.0074 - val_loss: 0.0577 - val_mse: 0.0056\n",
      "Epoch 39/200\n",
      "678/678 [==============================] - 54s 79ms/step - loss: 0.0617 - mse: 0.0070 - val_loss: 0.0789 - val_mse: 0.0097\n",
      "Epoch 40/200\n",
      "678/678 [==============================] - 56s 83ms/step - loss: 0.0616 - mse: 0.0070 - val_loss: 0.0680 - val_mse: 0.0084\n",
      "Epoch 41/200\n",
      "678/678 [==============================] - 56s 83ms/step - loss: 0.0618 - mse: 0.0071 - val_loss: 0.1047 - val_mse: 0.0188\n",
      "Epoch 42/200\n",
      "678/678 [==============================] - 56s 82ms/step - loss: 0.0586 - mse: 0.0064 - val_loss: 0.0792 - val_mse: 0.0099\n",
      "Epoch 43/200\n",
      "678/678 [==============================] - 56s 83ms/step - loss: 0.0603 - mse: 0.0066 - val_loss: 0.1006 - val_mse: 0.0148\n",
      "Epoch 44/200\n",
      "678/678 [==============================] - 54s 80ms/step - loss: 0.0586 - mse: 0.0063 - val_loss: 0.0768 - val_mse: 0.0104\n",
      "Epoch 45/200\n",
      "678/678 [==============================] - 57s 84ms/step - loss: 0.0591 - mse: 0.0063 - val_loss: 0.0752 - val_mse: 0.0104\n",
      "Epoch 46/200\n",
      "678/678 [==============================] - 56s 83ms/step - loss: 0.0574 - mse: 0.0060 - val_loss: 0.0709 - val_mse: 0.0086\n",
      "Epoch 47/200\n",
      "678/678 [==============================] - 56s 82ms/step - loss: 0.0566 - mse: 0.0059 - val_loss: 0.0728 - val_mse: 0.0103\n",
      "Epoch 48/200\n",
      "678/678 [==============================] - 59s 88ms/step - loss: 0.0554 - mse: 0.0055 - val_loss: 0.0765 - val_mse: 0.0103\n",
      "Epoch 49/200\n",
      "678/678 [==============================] - 56s 83ms/step - loss: 0.0557 - mse: 0.0058 - val_loss: 0.0735 - val_mse: 0.0099\n",
      "Epoch 50/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0547 - mse: 0.0054 - val_loss: 0.0718 - val_mse: 0.0090\n",
      "Epoch 51/200\n",
      "678/678 [==============================] - 52s 76ms/step - loss: 0.0541 - mse: 0.0053 - val_loss: 0.0817 - val_mse: 0.0112\n",
      "Epoch 52/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0531 - mse: 0.0052 - val_loss: 0.0742 - val_mse: 0.0090\n",
      "Epoch 53/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0536 - mse: 0.0052 - val_loss: 0.0758 - val_mse: 0.0091\n",
      "Epoch 54/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0516 - mse: 0.0049 - val_loss: 0.0672 - val_mse: 0.0075\n",
      "Epoch 55/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0507 - mse: 0.0047 - val_loss: 0.0587 - val_mse: 0.0062\n",
      "Epoch 56/200\n",
      "678/678 [==============================] - 53s 78ms/step - loss: 0.0519 - mse: 0.0048 - val_loss: 0.0759 - val_mse: 0.0102\n",
      "Epoch 57/200\n",
      "678/678 [==============================] - 53s 78ms/step - loss: 0.0503 - mse: 0.0046 - val_loss: 0.0692 - val_mse: 0.0081\n",
      "Epoch 58/200\n",
      "678/678 [==============================] - 53s 78ms/step - loss: 0.0498 - mse: 0.0046 - val_loss: 0.0771 - val_mse: 0.0107\n",
      "Epoch 59/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0490 - mse: 0.0046 - val_loss: 0.0730 - val_mse: 0.0088\n",
      "Epoch 60/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0491 - mse: 0.0044 - val_loss: 0.0618 - val_mse: 0.0065\n",
      "Epoch 61/200\n",
      "678/678 [==============================] - 52s 76ms/step - loss: 0.0492 - mse: 0.0045 - val_loss: 0.0644 - val_mse: 0.0068\n",
      "Epoch 62/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0471 - mse: 0.0041 - val_loss: 0.0676 - val_mse: 0.0080\n",
      "Epoch 63/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0489 - mse: 0.0044 - val_loss: 0.0578 - val_mse: 0.0059\n",
      "Epoch 64/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0471 - mse: 0.0040 - val_loss: 0.0687 - val_mse: 0.0087\n",
      "Epoch 65/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0479 - mse: 0.0042 - val_loss: 0.0677 - val_mse: 0.0078\n",
      "Epoch 66/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0472 - mse: 0.0043 - val_loss: 0.0668 - val_mse: 0.0068\n",
      "Epoch 67/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0460 - mse: 0.0040 - val_loss: 0.0586 - val_mse: 0.0062\n",
      "Epoch 68/200\n",
      "678/678 [==============================] - 52s 77ms/step - loss: 0.0468 - mse: 0.0041 - val_loss: 0.0779 - val_mse: 0.0098\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_6, test_size=0.2)\n",
    "\n",
    "model_6 = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(window_size_6, num_features)),\n",
    "    Dropout(0.3),\n",
    "    GRU(32, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(16, return_sequences=False),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "model_6.compile(optimizer=Adam(learning_rate=0.001),loss='mae',metrics=['mse'])\n",
    "# Train the model\n",
    "history_6 = model_6.fit(X_train,y_train,\n",
    "                   validation_data=(X_val,y_val),\n",
    "                    epochs=200, batch_size=16,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(patience=30,monitor='val_loss',restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 5ms/step\n",
      "window_len 5 model Model Evaluation - RMSE: 52.13, MAE: 39.62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-1.3000412 ],\n",
       "        [ 1.1116015 ],\n",
       "        [ 0.6796353 ],\n",
       "        ...,\n",
       "        [-1.3276074 ],\n",
       "        [ 0.21961956],\n",
       "        [-1.2511168 ]], dtype=float32),\n",
       " 52.13059984094697,\n",
       " 39.61638511267445)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_1, test_size=0.2)\n",
    "evaluate_model(model_1, X_test,y_test,f'window_len {window_size_1} model',yscaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 2s 7ms/step\n",
      "window_len 10 model Model Evaluation - RMSE: 23.73, MAE: 16.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.82974136],\n",
       "        [-0.9466406 ],\n",
       "        [-1.7531383 ],\n",
       "        ...,\n",
       "        [ 1.1574082 ],\n",
       "        [ 0.9325977 ],\n",
       "        [ 0.11034414]], dtype=float32),\n",
       " 23.73402431678229,\n",
       " 16.488184993878278)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_2, test_size=0.2)\n",
    "evaluate_model(model_2, X_test,y_test,f'window_len {window_size_2} model',yscaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 3s 11ms/step\n",
      "window_len 20 model Model Evaluation - RMSE: 9.00, MAE: 6.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.2991754 ],\n",
       "        [-0.5453476 ],\n",
       "        [-0.08779128],\n",
       "        ...,\n",
       "        [ 0.09589133],\n",
       "        [-0.95557123],\n",
       "        [-0.2911966 ]], dtype=float32),\n",
       " 8.996871776098274,\n",
       " 6.6653618534804355)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_3, test_size=0.2)\n",
    "evaluate_model(model_3, X_test,y_test,f'window_len {window_size_3} model',yscaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 3s 17ms/step\n",
      "window_len 30 model Model Evaluation - RMSE: 6.96, MAE: 5.09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.53057575],\n",
       "        [-1.279528  ],\n",
       "        [ 0.7114718 ],\n",
       "        ...,\n",
       "        [-0.8680728 ],\n",
       "        [ 0.84354126],\n",
       "        [-1.7484484 ]], dtype=float32),\n",
       " 6.962075660399104,\n",
       " 5.091556260758473)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_4, test_size=0.2)\n",
    "evaluate_model(model_4, X_test,y_test,f'window_len {window_size_4} model',yscaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\.conda\\envs\\mlEnv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:979: RuntimeWarning: invalid value encountered in sqrt\n",
      "  np.sqrt(self.var_), copy=False, constant_mask=constant_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 6s 24ms/step\n",
      "window_len 40 model Model Evaluation - RMSE: 5.59, MAE: 4.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.8582148],\n",
       "        [-0.5460262],\n",
       "        [-0.7912904],\n",
       "        ...,\n",
       "        [-0.7975193],\n",
       "        [-1.2992631],\n",
       "        [-1.8220927]], dtype=float32),\n",
       " 5.591231034703361,\n",
       " 4.330707889840527)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_5, test_size=0.2)\n",
    "evaluate_model(model_5, X_test,y_test,f'window_len {window_size_5} model',yscaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 4s 28ms/step\n",
      "window_len 50 model Model Evaluation - RMSE: 5.86, MAE: 4.31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.18080445],\n",
       "        [ 1.0308777 ],\n",
       "        [ 0.41731155],\n",
       "        ...,\n",
       "        [ 1.351302  ],\n",
       "        [-0.55051386],\n",
       "        [-0.58213365]], dtype=float32),\n",
       " 5.858551991722847,\n",
       " 4.310656302738159)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, scaler,yscaler= create_sequences_3(df, window_size=window_size_6, test_size=0.2)\n",
    "evaluate_model(model_6, X_test,y_test,f'window_len {window_size_6} model',yscaler)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
