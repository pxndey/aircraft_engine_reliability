{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth = pd.read_excel(\"D:\\\\Projects\\\\BDA\\\\aircraft_reliability\\\\data\\\\PM_truth.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>more</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20426</th>\n",
       "      <td>99</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.72</td>\n",
       "      <td>1600.39</td>\n",
       "      <td>1428.03</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.29</td>\n",
       "      <td>8123.55</td>\n",
       "      <td>8.4885</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.50</td>\n",
       "      <td>23.0425</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20427</th>\n",
       "      <td>99</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.52</td>\n",
       "      <td>1605.33</td>\n",
       "      <td>1430.32</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.27</td>\n",
       "      <td>8130.99</td>\n",
       "      <td>8.5124</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.39</td>\n",
       "      <td>22.9674</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20428</th>\n",
       "      <td>99</td>\n",
       "      <td>183</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.96</td>\n",
       "      <td>1606.95</td>\n",
       "      <td>1427.90</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.31</td>\n",
       "      <td>8126.90</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.57</td>\n",
       "      <td>23.1440</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20429</th>\n",
       "      <td>99</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>644.10</td>\n",
       "      <td>1600.20</td>\n",
       "      <td>1436.54</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.33</td>\n",
       "      <td>8125.66</td>\n",
       "      <td>8.5592</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.45</td>\n",
       "      <td>23.0478</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20430</th>\n",
       "      <td>99</td>\n",
       "      <td>185</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.93</td>\n",
       "      <td>1598.42</td>\n",
       "      <td>1421.56</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8127.53</td>\n",
       "      <td>8.5425</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.1931</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20431 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0       1      1   -0.0007   -0.0004       100  518.67  641.82  1589.70   \n",
       "1       1      2    0.0019   -0.0003       100  518.67  642.15  1591.82   \n",
       "2       1      3   -0.0043    0.0003       100  518.67  642.35  1587.99   \n",
       "3       1      4    0.0007    0.0000       100  518.67  642.35  1582.79   \n",
       "4       1      5   -0.0019   -0.0002       100  518.67  642.37  1582.85   \n",
       "...    ..    ...       ...       ...       ...     ...     ...      ...   \n",
       "20426  99    181   -0.0015   -0.0001       100  518.67  643.72  1600.39   \n",
       "20427  99    182   -0.0027   -0.0003       100  518.67  643.52  1605.33   \n",
       "20428  99    183   -0.0031   -0.0003       100  518.67  643.96  1606.95   \n",
       "20429  99    184   -0.0010   -0.0001       100  518.67  644.10  1600.20   \n",
       "20430  99    185   -0.0019   -0.0004       100  518.67  643.93  1598.42   \n",
       "\n",
       "            s4     s5  ...      s13      s14     s15   s16  s17   s18  s19  \\\n",
       "0      1400.60  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100   \n",
       "1      1403.14  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100   \n",
       "2      1404.20  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100   \n",
       "3      1401.87  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100   \n",
       "4      1406.22  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100   \n",
       "...        ...    ...  ...      ...      ...     ...   ...  ...   ...  ...   \n",
       "20426  1428.03  14.62  ...  2388.29  8123.55  8.4885  0.03  396  2388  100   \n",
       "20427  1430.32  14.62  ...  2388.27  8130.99  8.5124  0.03  393  2388  100   \n",
       "20428  1427.90  14.62  ...  2388.31  8126.90  8.5374  0.03  395  2388  100   \n",
       "20429  1436.54  14.62  ...  2388.33  8125.66  8.5592  0.03  395  2388  100   \n",
       "20430  1421.56  14.62  ...  2388.24  8127.53  8.5425  0.03  397  2388  100   \n",
       "\n",
       "         s20      s21  more  \n",
       "0      39.06  23.4190    98  \n",
       "1      39.00  23.4236    98  \n",
       "2      38.95  23.3442    98  \n",
       "3      38.88  23.3739    98  \n",
       "4      38.90  23.4044    98  \n",
       "...      ...      ...   ...  \n",
       "20426  38.50  23.0425    20  \n",
       "20427  38.39  22.9674    20  \n",
       "20428  38.57  23.1440    20  \n",
       "20429  38.45  23.0478    20  \n",
       "20430  38.49  23.1931    20  \n",
       "\n",
       "[20431 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.merge(df, df_truth, on='id')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the maximum cycle for each engine\n",
    "max_cycle_per_engine = df.groupby('id')['cycle'].max().reset_index()\n",
    "max_cycle_per_engine.columns = ['id', 'max_cycle']\n",
    "\n",
    "# Step 2: Merge the maximum cycle with the df_truth to get the actual failure cycle\n",
    "df_merged = pd.merge(max_cycle_per_engine, df_truth, on='id')\n",
    "\n",
    "# Step 3: Calculate the actual failure cycle (when engine will fail)\n",
    "df_merged['failure_cycle'] = df_merged['max_cycle'] + df_merged['more']\n",
    "\n",
    "# Step 4: Merge this back with the main DataFrame to compute remaining cycles\n",
    "df = pd.merge(df, df_merged[['id', 'failure_cycle']], on='id')\n",
    "\n",
    "# Step 5: Calculate remaining cycles for each row by subtracting the current cycle from the failure cycle\n",
    "df['remaining_cycles'] = df['failure_cycle'] - df['cycle']\n",
    "df = df.drop('failure_cycle',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s13</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>remaining_cycles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20426</th>\n",
       "      <td>99</td>\n",
       "      <td>181</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.72</td>\n",
       "      <td>1600.39</td>\n",
       "      <td>1428.03</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.29</td>\n",
       "      <td>8123.55</td>\n",
       "      <td>8.4885</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.50</td>\n",
       "      <td>23.0425</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20427</th>\n",
       "      <td>99</td>\n",
       "      <td>182</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.52</td>\n",
       "      <td>1605.33</td>\n",
       "      <td>1430.32</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.27</td>\n",
       "      <td>8130.99</td>\n",
       "      <td>8.5124</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.39</td>\n",
       "      <td>22.9674</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20428</th>\n",
       "      <td>99</td>\n",
       "      <td>183</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.96</td>\n",
       "      <td>1606.95</td>\n",
       "      <td>1427.90</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.31</td>\n",
       "      <td>8126.90</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.57</td>\n",
       "      <td>23.1440</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20429</th>\n",
       "      <td>99</td>\n",
       "      <td>184</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>644.10</td>\n",
       "      <td>1600.20</td>\n",
       "      <td>1436.54</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.33</td>\n",
       "      <td>8125.66</td>\n",
       "      <td>8.5592</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.45</td>\n",
       "      <td>23.0478</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20430</th>\n",
       "      <td>99</td>\n",
       "      <td>185</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100</td>\n",
       "      <td>518.67</td>\n",
       "      <td>643.93</td>\n",
       "      <td>1598.42</td>\n",
       "      <td>1421.56</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>2388.24</td>\n",
       "      <td>8127.53</td>\n",
       "      <td>8.5425</td>\n",
       "      <td>0.03</td>\n",
       "      <td>397</td>\n",
       "      <td>2388</td>\n",
       "      <td>100</td>\n",
       "      <td>38.49</td>\n",
       "      <td>23.1931</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20431 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
       "0       1      1   -0.0007   -0.0004       100  518.67  641.82  1589.70   \n",
       "1       1      2    0.0019   -0.0003       100  518.67  642.15  1591.82   \n",
       "2       1      3   -0.0043    0.0003       100  518.67  642.35  1587.99   \n",
       "3       1      4    0.0007    0.0000       100  518.67  642.35  1582.79   \n",
       "4       1      5   -0.0019   -0.0002       100  518.67  642.37  1582.85   \n",
       "...    ..    ...       ...       ...       ...     ...     ...      ...   \n",
       "20426  99    181   -0.0015   -0.0001       100  518.67  643.72  1600.39   \n",
       "20427  99    182   -0.0027   -0.0003       100  518.67  643.52  1605.33   \n",
       "20428  99    183   -0.0031   -0.0003       100  518.67  643.96  1606.95   \n",
       "20429  99    184   -0.0010   -0.0001       100  518.67  644.10  1600.20   \n",
       "20430  99    185   -0.0019   -0.0004       100  518.67  643.93  1598.42   \n",
       "\n",
       "            s4     s5  ...      s13      s14     s15   s16  s17   s18  s19  \\\n",
       "0      1400.60  14.62  ...  2388.02  8138.62  8.4195  0.03  392  2388  100   \n",
       "1      1403.14  14.62  ...  2388.07  8131.49  8.4318  0.03  392  2388  100   \n",
       "2      1404.20  14.62  ...  2388.03  8133.23  8.4178  0.03  390  2388  100   \n",
       "3      1401.87  14.62  ...  2388.08  8133.83  8.3682  0.03  392  2388  100   \n",
       "4      1406.22  14.62  ...  2388.04  8133.80  8.4294  0.03  393  2388  100   \n",
       "...        ...    ...  ...      ...      ...     ...   ...  ...   ...  ...   \n",
       "20426  1428.03  14.62  ...  2388.29  8123.55  8.4885  0.03  396  2388  100   \n",
       "20427  1430.32  14.62  ...  2388.27  8130.99  8.5124  0.03  393  2388  100   \n",
       "20428  1427.90  14.62  ...  2388.31  8126.90  8.5374  0.03  395  2388  100   \n",
       "20429  1436.54  14.62  ...  2388.33  8125.66  8.5592  0.03  395  2388  100   \n",
       "20430  1421.56  14.62  ...  2388.24  8127.53  8.5425  0.03  397  2388  100   \n",
       "\n",
       "         s20      s21  remaining_cycles  \n",
       "0      39.06  23.4190               289  \n",
       "1      39.00  23.4236               288  \n",
       "2      38.95  23.3442               287  \n",
       "3      38.88  23.3739               286  \n",
       "4      38.90  23.4044               285  \n",
       "...      ...      ...               ...  \n",
       "20426  38.50  23.0425                24  \n",
       "20427  38.39  22.9674                23  \n",
       "20428  38.57  23.1440                22  \n",
       "20429  38.45  23.0478                21  \n",
       "20430  38.49  23.1931                20  \n",
       "\n",
       "[20431 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short-term sequences shape: (19540, 10, 24)\n",
      "Short-term targets shape: (19540,)\n",
      "Medium-term sequences shape: (15580, 10, 24)\n",
      "Medium-term targets shape: (15580,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, sequence_length, downsample=1):\n",
    "    \"\"\"\n",
    "    Creates sequences of given length, with optional downsampling.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data\n",
    "    - sequence_length: The length of the sequences\n",
    "    - downsample: Factor by which to downsample within the sequence\n",
    "    \n",
    "    Returns:\n",
    "    - List of sequences (each sequence is an array of shape [sequence_length, features])\n",
    "    - List of corresponding remaining_cycles (targets for each sequence)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    # Group by 'id' to handle each engine separately\n",
    "    for engine_id in data['id'].unique():\n",
    "        engine_data = data[data['id'] == engine_id]\n",
    "\n",
    "        # Loop through each cycle to create sequences\n",
    "        for i in range(len(engine_data) - sequence_length + 1):\n",
    "            # Extract the sequence and downsample if required\n",
    "            sequence = engine_data.iloc[i:i + sequence_length].iloc[::downsample]\n",
    "            target = engine_data['remaining_cycles'].iloc[i + sequence_length - 1]\n",
    "            \n",
    "            sequences.append(sequence.drop(columns=['id', 'cycle', 'remaining_cycles']).values)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Short-term sequences (no downsampling)\n",
    "short_term_sequences, short_term_targets = create_sequences(data, sequence_length=10, downsample=1)\n",
    "\n",
    "# Medium-term sequences (downsample by a factor of 5 to reduce length)\n",
    "medium_term_sequences, medium_term_targets = create_sequences(data, sequence_length=50, downsample=5)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Short-term sequences shape:\", short_term_sequences.shape)\n",
    "print(\"Short-term targets shape:\", short_term_targets.shape)\n",
    "print(\"Medium-term sequences shape:\", medium_term_sequences.shape)\n",
    "print(\"Medium-term targets shape:\", medium_term_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short-term sequences shape: (15580, 10, 24)\n",
      "Short-term targets shape: (19540,)\n",
      "Medium-term sequences shape: (15580, 10, 24)\n",
      "Medium-term targets shape: (15580,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define function to create sequences for different scales\n",
    "def create_aligned_sequences(data, short_term_len, medium_term_len, downsample=1):\n",
    "    short_term_sequences = []\n",
    "    medium_term_sequences = []\n",
    "    targets = []\n",
    "\n",
    "    for engine_id in data['id'].unique():\n",
    "        engine_data = data[data['id'] == engine_id]\n",
    "\n",
    "        # Minimum range to allow both short- and medium-term sequences to fit\n",
    "        max_start = len(engine_data) - max(short_term_len, medium_term_len) + 1\n",
    "\n",
    "        for i in range(max_start):\n",
    "            # Short-term sequence (no downsampling)\n",
    "            short_sequence = engine_data.iloc[i:i + short_term_len]\n",
    "            # Medium-term sequence with downsampling\n",
    "            medium_sequence = engine_data.iloc[i:i + medium_term_len].iloc[::downsample]\n",
    "\n",
    "            # Ensure the lengths are as expected\n",
    "            if len(short_sequence) == short_term_len and len(medium_sequence) == (medium_term_len // downsample):\n",
    "                short_term_sequences.append(short_sequence.drop(columns=['id', 'cycle', 'remaining_cycles']).values)\n",
    "                medium_term_sequences.append(medium_sequence.drop(columns=['id', 'cycle', 'remaining_cycles']).values)\n",
    "                # Target at the end of the sequence\n",
    "                targets.append(engine_data['remaining_cycles'].iloc[i + short_term_len - 1])\n",
    "\n",
    "    return np.array(short_term_sequences), np.array(medium_term_sequences), np.array(targets)\n",
    "\n",
    "# Generate aligned sequences\n",
    "short_term_sequences, medium_term_sequences, targets = create_aligned_sequences(\n",
    "    data, short_term_len=10, medium_term_len=50, downsample=5\n",
    ")\n",
    "\n",
    "\n",
    "# Check shapes\n",
    "print(\"Short-term sequences shape:\", short_term_sequences.shape)\n",
    "print(\"Short-term targets shape:\", short_term_targets.shape)\n",
    "print(\"Medium-term sequences shape:\", medium_term_sequences.shape)\n",
    "print(\"Medium-term targets shape:\", medium_term_targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate models for short-term and medium-term predictions\n",
    "def create_short_term_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    lstm_layer = LSTM(128, return_sequences=True)(input_layer)\n",
    "    lstm_layer = Dropout(0.3)(lstm_layer)\n",
    "    lstm_layer = LSTM(64)(lstm_layer)\n",
    "    output_layer = Dense(1)(lstm_layer)\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "def create_medium_term_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    lstm_layer = LSTM(128, return_sequences=True)(input_layer)\n",
    "    lstm_layer = Dropout(0.3)(lstm_layer)\n",
    "    lstm_layer = LSTM(64)(lstm_layer)\n",
    "    output_layer = Dense(1)(lstm_layer)\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create and compile the models\n",
    "short_term_model = create_short_term_model(short_term_sequences.shape[1:])\n",
    "medium_term_model = create_medium_term_model(medium_term_sequences.shape[1:])\n",
    "\n",
    "short_term_model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])\n",
    "medium_term_model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mae'])\n",
    "\n",
    "# Compile the model with a lower learning rate and MAE loss\n",
    "initial_learning_rate = 0.00001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "\n",
    "# Implement learning rate schedule\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: initial_learning_rate * (0.1 ** (epoch // 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "195/195 [==============================] - 8s 22ms/step - loss: 172.5944 - mae: 172.5944 - val_loss: 180.2815 - val_mae: 180.2815 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 172.0046 - mae: 172.0046 - val_loss: 179.5676 - val_mae: 179.5676 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "195/195 [==============================] - 3s 16ms/step - loss: 171.2340 - mae: 171.2340 - val_loss: 178.6143 - val_mae: 178.6143 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 170.2404 - mae: 170.2404 - val_loss: 177.5325 - val_mae: 177.5325 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 169.1969 - mae: 169.1969 - val_loss: 176.5832 - val_mae: 176.5832 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 168.3382 - mae: 168.3382 - val_loss: 175.8645 - val_mae: 175.8645 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 167.6654 - mae: 167.6654 - val_loss: 175.2780 - val_mae: 175.2780 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 167.1085 - mae: 167.1085 - val_loss: 174.7877 - val_mae: 174.7877 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 166.6449 - mae: 166.6449 - val_loss: 174.3689 - val_mae: 174.3689 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 166.2343 - mae: 166.2343 - val_loss: 173.9792 - val_mae: 173.9792 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 166.0155 - mae: 166.0155 - val_loss: 173.9420 - val_mae: 173.9420 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.9772 - mae: 165.9772 - val_loss: 173.9060 - val_mae: 173.9060 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.9418 - mae: 165.9418 - val_loss: 173.8712 - val_mae: 173.8712 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.9061 - mae: 165.9061 - val_loss: 173.8370 - val_mae: 173.8370 - lr: 1.0000e-06\n",
      "Epoch 15/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.8721 - mae: 165.8721 - val_loss: 173.8033 - val_mae: 173.8033 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.8384 - mae: 165.8384 - val_loss: 173.7698 - val_mae: 173.7698 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.8026 - mae: 165.8026 - val_loss: 173.7369 - val_mae: 173.7369 - lr: 1.0000e-06\n",
      "Epoch 18/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.7698 - mae: 165.7698 - val_loss: 173.7042 - val_mae: 173.7042 - lr: 1.0000e-06\n",
      "Epoch 19/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.7364 - mae: 165.7364 - val_loss: 173.6716 - val_mae: 173.6716 - lr: 1.0000e-06\n",
      "Epoch 20/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.7043 - mae: 165.7043 - val_loss: 173.6393 - val_mae: 173.6393 - lr: 1.0000e-06\n",
      "Epoch 21/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.6854 - mae: 165.6854 - val_loss: 173.6363 - val_mae: 173.6363 - lr: 1.0000e-07\n",
      "Epoch 22/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.6818 - mae: 165.6818 - val_loss: 173.6330 - val_mae: 173.6330 - lr: 1.0000e-07\n",
      "Epoch 23/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.6780 - mae: 165.6780 - val_loss: 173.6297 - val_mae: 173.6297 - lr: 1.0000e-07\n",
      "Epoch 24/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6760 - mae: 165.6760 - val_loss: 173.6266 - val_mae: 173.6266 - lr: 1.0000e-07\n",
      "Epoch 25/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6723 - mae: 165.6723 - val_loss: 173.6233 - val_mae: 173.6233 - lr: 1.0000e-07\n",
      "Epoch 26/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6692 - mae: 165.6692 - val_loss: 173.6202 - val_mae: 173.6202 - lr: 1.0000e-07\n",
      "Epoch 27/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6640 - mae: 165.6640 - val_loss: 173.6169 - val_mae: 173.6169 - lr: 1.0000e-07\n",
      "Epoch 28/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.6614 - mae: 165.6614 - val_loss: 173.6135 - val_mae: 173.6135 - lr: 1.0000e-07\n",
      "Epoch 29/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6572 - mae: 165.6572 - val_loss: 173.6104 - val_mae: 173.6104 - lr: 1.0000e-07\n",
      "Epoch 30/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6552 - mae: 165.6552 - val_loss: 173.6071 - val_mae: 173.6071 - lr: 1.0000e-07\n",
      "Epoch 31/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.6517 - mae: 165.6517 - val_loss: 173.6067 - val_mae: 173.6067 - lr: 1.0000e-08\n",
      "Epoch 32/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6528 - mae: 165.6528 - val_loss: 173.6065 - val_mae: 173.6065 - lr: 1.0000e-08\n",
      "Epoch 33/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6524 - mae: 165.6524 - val_loss: 173.6061 - val_mae: 173.6061 - lr: 1.0000e-08\n",
      "Epoch 34/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6507 - mae: 165.6507 - val_loss: 173.6057 - val_mae: 173.6057 - lr: 1.0000e-08\n",
      "Epoch 35/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6508 - mae: 165.6508 - val_loss: 173.6055 - val_mae: 173.6055 - lr: 1.0000e-08\n",
      "Epoch 36/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.6499 - mae: 165.6499 - val_loss: 173.6051 - val_mae: 173.6051 - lr: 1.0000e-08\n",
      "Epoch 37/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6510 - mae: 165.6510 - val_loss: 173.6048 - val_mae: 173.6048 - lr: 1.0000e-08\n",
      "Epoch 38/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6505 - mae: 165.6505 - val_loss: 173.6046 - val_mae: 173.6046 - lr: 1.0000e-08\n",
      "Epoch 39/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6502 - mae: 165.6502 - val_loss: 173.6042 - val_mae: 173.6042 - lr: 1.0000e-08\n",
      "Epoch 40/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6497 - mae: 165.6497 - val_loss: 173.6038 - val_mae: 173.6038 - lr: 1.0000e-08\n",
      "Epoch 41/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6038 - val_mae: 173.6038 - lr: 1.0000e-09\n",
      "Epoch 42/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6498 - mae: 165.6498 - val_loss: 173.6038 - val_mae: 173.6038 - lr: 1.0000e-09\n",
      "Epoch 43/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6496 - mae: 165.6496 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 44/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6485 - mae: 165.6485 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 45/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6487 - mae: 165.6487 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 46/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6496 - mae: 165.6496 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 47/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6499 - mae: 165.6499 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 48/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6500 - mae: 165.6500 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 49/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6501 - mae: 165.6501 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 50/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6490 - mae: 165.6490 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-09\n",
      "Epoch 51/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6491 - mae: 165.6491 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 52/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 53/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 165.6491 - mae: 165.6491 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 54/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6487 - mae: 165.6487 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 55/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6493 - mae: 165.6493 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 56/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6495 - mae: 165.6495 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 57/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6503 - mae: 165.6503 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 58/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6493 - mae: 165.6493 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 59/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6496 - mae: 165.6496 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 60/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6493 - mae: 165.6493 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-10\n",
      "Epoch 61/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6499 - mae: 165.6499 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 62/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6501 - mae: 165.6501 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 63/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6504 - mae: 165.6504 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 64/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 65/100\n",
      "195/195 [==============================] - 4s 20ms/step - loss: 165.6490 - mae: 165.6490 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 66/100\n",
      "195/195 [==============================] - 4s 20ms/step - loss: 165.6488 - mae: 165.6488 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 67/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6498 - mae: 165.6498 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 68/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6487 - mae: 165.6487 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 69/100\n",
      "195/195 [==============================] - 4s 20ms/step - loss: 165.6498 - mae: 165.6498 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 70/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6480 - mae: 165.6480 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-11\n",
      "Epoch 71/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6492 - mae: 165.6492 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 72/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6505 - mae: 165.6505 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 73/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 74/100\n",
      "195/195 [==============================] - 4s 19ms/step - loss: 165.6488 - mae: 165.6488 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 75/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6491 - mae: 165.6491 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 76/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6504 - mae: 165.6504 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 77/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6491 - mae: 165.6491 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 78/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 79/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6489 - mae: 165.6489 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 80/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-12\n",
      "Epoch 81/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6475 - mae: 165.6475 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 82/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6503 - mae: 165.6503 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 83/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6490 - mae: 165.6490 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 84/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6499 - mae: 165.6499 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 85/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 86/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6498 - mae: 165.6498 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 87/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6475 - mae: 165.6475 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 88/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6485 - mae: 165.6485 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 89/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6486 - mae: 165.6486 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 90/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6488 - mae: 165.6488 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-13\n",
      "Epoch 91/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6495 - mae: 165.6495 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 92/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6492 - mae: 165.6492 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 93/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6490 - mae: 165.6490 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 94/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6501 - mae: 165.6501 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 95/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6494 - mae: 165.6494 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 96/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6496 - mae: 165.6496 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 97/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6498 - mae: 165.6498 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 98/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6503 - mae: 165.6503 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 99/100\n",
      "195/195 [==============================] - 4s 18ms/step - loss: 165.6489 - mae: 165.6489 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 100/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 165.6497 - mae: 165.6497 - val_loss: 173.6037 - val_mae: 173.6037 - lr: 1.0000e-14\n",
      "Epoch 1/100\n",
      "195/195 [==============================] - 8s 22ms/step - loss: 155.5701 - mae: 155.5701 - val_loss: 186.6983 - val_mae: 186.6983 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "195/195 [==============================] - 3s 17ms/step - loss: 154.8185 - mae: 154.8185 - val_loss: 185.9331 - val_mae: 185.9331 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "195/195 [==============================] - 3s 18ms/step - loss: 154.0446 - mae: 154.0446 - val_loss: 185.0733 - val_mae: 185.0733 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "195/195 [==============================] - 4s 20ms/step - loss: 153.1408 - mae: 153.1408 - val_loss: 184.0396 - val_mae: 184.0396 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "  8/195 [>.............................] - ETA: 3s - loss: 150.4559 - mae: 150.4559"
     ]
    }
   ],
   "source": [
    "# Train each model separately\n",
    "short_term_history = short_term_model.fit(\n",
    "    short_term_sequences,\n",
    "    short_term_targets,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[lr_schedule]\n",
    ")\n",
    "\n",
    "medium_term_history = medium_term_model.fit(\n",
    "    medium_term_sequences,\n",
    "    medium_term_targets,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[lr_schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get unique engine IDs\n",
    "engine_ids = data['id'].unique()\n",
    "\n",
    "# Split engine IDs into training and testing sets (e.g., 80% train, 20% test)\n",
    "train_ids, test_ids = train_test_split(engine_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter data for train and test sets\n",
    "train_data = data[data['id'].isin(train_ids)]\n",
    "test_data = data[data['id'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences for training\n",
    "short_term_train, medium_term_train, train_targets = create_aligned_sequences(\n",
    "    train_data, short_term_len=10, medium_term_len=50, downsample=5\n",
    ")\n",
    "\n",
    "# Generate sequences for testing\n",
    "short_term_test, medium_term_test, test_targets = create_aligned_sequences(\n",
    "    test_data, short_term_len=10, medium_term_len=50, downsample=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with both models\n",
    "short_term_predictions = short_term_model.predict(short_term_test_sequences)\n",
    "medium_term_predictions = medium_term_model.predict(medium_term_test_sequences)\n",
    "\n",
    "# Average the predictions\n",
    "final_predictions = (short_term_predictions.flatten() + medium_term_predictions.flatten()) / 2\n",
    "\n",
    "# Compare predictions with actual values\n",
    "for i in range(10):\n",
    "    print(f\"Predicted: {final_predictions[i]:.2f}, Actual: {actual_rul[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
